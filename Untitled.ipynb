{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac75346-91cf-444e-a547-58c428d55ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recipe-scrapers\n",
      "  Downloading recipe_scrapers-15.6.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.3 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from recipe-scrapers) (4.12.3)\n",
      "Collecting extruct>=0.17.0 (from recipe-scrapers)\n",
      "  Downloading extruct-0.18.0-py2.py3-none-any.whl.metadata (36 kB)\n",
      "Collecting isodate>=0.6.1 (from recipe-scrapers)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.12.3->recipe-scrapers) (2.5)\n",
      "Requirement already satisfied: lxml in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from extruct>=0.17.0->recipe-scrapers) (5.2.1)\n",
      "Collecting lxml-html-clean (from extruct>=0.17.0->recipe-scrapers)\n",
      "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting rdflib>=6.0.0 (from extruct>=0.17.0->recipe-scrapers)\n",
      "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyrdfa3 (from extruct>=0.17.0->recipe-scrapers)\n",
      "  Downloading pyRdfa3-3.6.4-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting mf2py (from extruct>=0.17.0->recipe-scrapers)\n",
      "  Downloading mf2py-2.0.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: w3lib in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from extruct>=0.17.0->recipe-scrapers) (2.1.2)\n",
      "Collecting html-text>=0.5.1 (from extruct>=0.17.0->recipe-scrapers)\n",
      "  Downloading html_text-0.7.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting jstyleson (from extruct>=0.17.0->recipe-scrapers)\n",
      "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from rdflib>=6.0.0->extruct>=0.17.0->recipe-scrapers) (3.0.9)\n",
      "Collecting html5lib<2.0,>=1.1 (from mf2py->extruct>=0.17.0->recipe-scrapers)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.2 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from mf2py->extruct>=0.17.0->recipe-scrapers) (2.32.2)\n",
      "Collecting requests<3.0.0,>=2.28.2 (from mf2py->extruct>=0.17.0->recipe-scrapers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.9 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from html5lib<2.0,>=1.1->mf2py->extruct>=0.17.0->recipe-scrapers) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from html5lib<2.0,>=1.1->mf2py->extruct>=0.17.0->recipe-scrapers) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.2->mf2py->extruct>=0.17.0->recipe-scrapers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.2->mf2py->extruct>=0.17.0->recipe-scrapers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.2->mf2py->extruct>=0.17.0->recipe-scrapers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.2->mf2py->extruct>=0.17.0->recipe-scrapers) (2024.6.2)\n",
      "Downloading recipe_scrapers-15.6.0-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading extruct-0.18.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Downloading html_text-0.7.0-py3-none-any.whl (8.1 kB)\n",
      "Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading mf2py-2.0.1-py3-none-any.whl (25 kB)\n",
      "Downloading pyRdfa3-3.6.4-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: jstyleson\n",
      "  Building wheel for jstyleson (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2385 sha256=169002ca008d912565c5fa846dde25ceb0d8fdad2f7d767d8996eae177572c53\n",
      "  Stored in directory: /Users/shivanishah/Library/Caches/pip/wheels/13/7a/c7/3dd47109d94260d97a457d1ab4402f4b324af6f8e8b2aa7481\n",
      "Successfully built jstyleson\n",
      "Installing collected packages: jstyleson, requests, rdflib, lxml-html-clean, isodate, html5lib, pyrdfa3, mf2py, html-text, extruct, recipe-scrapers\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.2\n",
      "    Uninstalling requests-2.32.2:\n",
      "      Successfully uninstalled requests-2.32.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.5.1 requires pylint<3.1,>=2.5.0, but you have pylint 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed extruct-0.18.0 html-text-0.7.0 html5lib-1.1 isodate-0.7.2 jstyleson-0.0.2 lxml-html-clean-0.4.1 mf2py-2.0.1 pyrdfa3-3.6.4 rdflib-7.1.4 recipe-scrapers-15.6.0 requests-2.32.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install recipe-scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06c634b2-8446-49e0-83a8-04dedb175dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting playwright\n",
      "  Downloading playwright-1.51.0-py3-none-macosx_10_13_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting pyee<13,>=12 (from playwright)\n",
      "  Downloading pyee-12.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting greenlet<4.0.0,>=3.1.1 (from playwright)\n",
      "  Downloading greenlet-3.1.1.tar.gz (186 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m186.0/186.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/shivanishah/anaconda3/lib/python3.12/site-packages (from pyee<13,>=12->playwright) (4.11.0)\n",
      "Downloading playwright-1.51.0-py3-none-macosx_10_13_x86_64.whl (39.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.6/39.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyee-12.1.1-py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: greenlet\n",
      "  Building wheel for greenlet (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for greenlet: filename=greenlet-3.1.1-cp312-cp312-macosx_10_15_x86_64.whl size=222334 sha256=dcccfb62b42614e06b0944865d5c8f14957195d9ef882373430e7d28d8571fc7\n",
      "  Stored in directory: /Users/shivanishah/Library/Caches/pip/wheels/15/a9/6e/f08ac286dedfab159aef551a5a7937300a0bac68ab44c3c334\n",
      "Successfully built greenlet\n",
      "Installing collected packages: pyee, greenlet, playwright\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 3.0.1\n",
      "    Uninstalling greenlet-3.0.1:\n",
      "      Successfully uninstalled greenlet-3.0.1\n",
      "Successfully installed greenlet-3.1.1 playwright-1.51.0 pyee-12.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install playwright\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "685cfbfd-46f1-4e80-b0bd-ead3b8875a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1703f523-24ba-42b0-80e7-de3531f412a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from recipe_scrapers import scrape_html\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6405aaca-d5a5-48a0-a3ad-021aaf2f7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b150c83c-1e0d-4ec2-94ce-28aa1d7ef18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"3bbd49857cbba25dae2b4e16e2e81ecd2d2c68041fd4c028735cb92d8430422d\"\n",
    "\n",
    "def get_first_10(query):\n",
    "    params = {\n",
    "    \"engine\": \"google\",\n",
    "    \"q\": query,\n",
    "    \"num\": 10,\n",
    "    \"api_key\": api_key}    \n",
    "    res = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "    results = res.json()\n",
    "    urls = [r['link'] for r in results.get('organic_results', [])]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ed4d0d-8bd0-4515-ab99-dc77458e048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = get_first_10(\"chocolate chip cookie recipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18119210-43ee-4788-add2-6fcb308a81f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://joyfoodsunshine.com/the-most-amazing-chocolate-chip-cookies/', 'https://pinchofyum.com/the-best-soft-chocolate-chip-cookies', 'https://www.allrecipes.com/recipe/10909/annas-chocolate-chip-cookies/', 'https://www.nestle.com/stories/timeless-discovery-toll-house-chocolate-chip-cookie-recipe', 'https://sallysbakingaddiction.com/chewy-chocolate-chip-cookies/', 'https://www.reddit.com/r/BakingNoobs/comments/1hsdefn/does_anyone_have_any_good_chocolate_chip_cookie/', 'https://preppykitchen.com/chewy-chocolate-chip-cookies/', 'https://www.verybestbaking.com/toll-house/recipes/original-nestle-toll-house-chocolate-chip-cookies/']\n"
     ]
    }
   ],
   "source": [
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "272b3382-b17d-4fcf-97e8-a0fb99b4c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def fetch_html_and_scrape(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Will raise if 403, etc.\n",
    "    scraper = scrape_html(response.text, url)\n",
    "    return {\n",
    "        \"Title\": scraper.title(),\n",
    "        \"Ingredients\": ', '.join(scraper.ingredients()),\n",
    "        \"Instructions\": scraper.instructions(),\n",
    "        \"URL\": url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c21dc3d5-8c19-4977-acea-6d7e41cd93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipes(urls):\n",
    "    recipes = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            recipe = fetch_html_and_scrape(url)\n",
    "            recipes.append(recipe)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4150f32-efc6-4a3f-bcc3-acc85b60fcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape https://www.nestle.com/stories/timeless-discovery-toll-house-chocolate-chip-cookie-recipe: 403 Client Error: Forbidden for url: https://www.nestle.com/stories/timeless-discovery-toll-house-chocolate-chip-cookie-recipe\n",
      "Failed to scrape https://www.reddit.com/r/BakingNoobs/comments/1hsdefn/does_anyone_have_any_good_chocolate_chip_cookie/: recipe-scrapers exception: Website (The website 'reddit.com' isn't currently supported by recipe-scrapers!\n",
      "---\n",
      "If you have time to help us out, please report this as a feature \n",
      "request on our bugtracker.) not supported.\n",
      "Failed to scrape https://www.verybestbaking.com/toll-house/recipes/original-nestle-toll-house-chocolate-chip-cookies/: 403 Client Error: Forbidden for url: https://www.verybestbaking.com/toll-house/recipes/original-nestle-toll-house-chocolate-chip-cookies/\n"
     ]
    }
   ],
   "source": [
    "recipes(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e6f415b-e549-4646-96f1-debe115f22aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_recipe_jsonld(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        scripts = soup.find_all('script', type='application/ld+json')\n",
    "        for script in scripts:\n",
    "            try:\n",
    "                data = json.loads(script.string)\n",
    "                # Check if it's a Recipe\n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if item.get(\"@type\") == \"Recipe\":\n",
    "                            return item\n",
    "                elif data.get(\"@type\") == \"Recipe\":\n",
    "                    return data\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to fetch {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def parse_recipe(data, url):\n",
    "    return {\n",
    "        \"Title\": data.get(\"name\"),\n",
    "        \"Ingredients\": ', '.join(data.get(\"recipeIngredient\", [])),\n",
    "        \"Instructions\": (\n",
    "            ' '.join(\n",
    "                step.get(\"text\", \"\") for step in data.get(\"recipeInstructions\", [])\n",
    "                if isinstance(step, dict)\n",
    "            ) if isinstance(data.get(\"recipeInstructions\"), list)\n",
    "            else data.get(\"recipeInstructions\", \"\")\n",
    "        ),\n",
    "        \"URL\": url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "669e79f5-4300-4cc5-8c27-3dd8528c30ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No recipe data found on: https://joyfoodsunshine.com/the-most-amazing-chocolate-chip-cookies/\n",
      "âš ï¸ No recipe data found on: https://pinchofyum.com/the-best-soft-chocolate-chip-cookies\n",
      "âš ï¸ No recipe data found on: https://www.allrecipes.com/recipe/10909/annas-chocolate-chip-cookies/\n",
      "âš ï¸ No recipe data found on: https://www.nestle.com/stories/timeless-discovery-toll-house-chocolate-chip-cookie-recipe\n",
      "âš ï¸ No recipe data found on: https://sallysbakingaddiction.com/chewy-chocolate-chip-cookies/\n",
      "âš ï¸ No recipe data found on: https://www.reddit.com/r/BakingNoobs/comments/1hsdefn/does_anyone_have_any_good_chocolate_chip_cookie/\n",
      "âš ï¸ No recipe data found on: https://preppykitchen.com/chewy-chocolate-chip-cookies/\n",
      "âš ï¸ No recipe data found on: https://www.verybestbaking.com/toll-house/recipes/original-nestle-toll-house-chocolate-chip-cookies/\n"
     ]
    }
   ],
   "source": [
    "recipes = []\n",
    "for url in urls:\n",
    "    data = extract_recipe_jsonld(url)\n",
    "    if data:\n",
    "        recipe = parse_recipe(data, url)\n",
    "        recipes.append(recipe)\n",
    "    else:\n",
    "        print(f\"âš ï¸ No recipe data found on: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b7b2a5d-3e31-4134-8fe4-364154d94b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error scraping https://joyfoodsunshine.com/the-most-amazing-chocolate-chip-cookies/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "âŒ Error scraping https://pinchofyum.com/the-best-soft-chocolate-chip-cookies: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "âŒ Error scraping https://www.allrecipes.com/recipe/10909/annas-chocolate-chip-cookies/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "âŒ Error scraping https://www.nestle.com/stories/timeless-discovery-toll-house-chocolate-chip-cookie-recipe: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "âŒ Error scraping https://sallysbakingaddiction.com/chewy-chocolate-chip-cookies/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "âŒ Error scraping https://www.reddit.com/r/BakingNoobs/comments/1hsdefn/does_anyone_have_any_good_chocolate_chip_cookie/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "âŒ Error scraping https://preppykitchen.com/chewy-chocolate-chip-cookies/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "âŒ Error scraping https://www.verybestbaking.com/toll-house/recipes/original-nestle-toll-house-chocolate-chip-cookies/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n"
     ]
    }
   ],
   "source": [
    "def extract_recipe_with_playwright(url):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        page.goto(url, timeout=30000)\n",
    "        html = page.content()\n",
    "        browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    scripts = soup.find_all('script', type='application/ld+json')\n",
    "    \n",
    "    for script in scripts:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if item.get(\"@type\") == \"Recipe\":\n",
    "                        return item\n",
    "            elif data.get(\"@type\") == \"Recipe\":\n",
    "                return data\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def parse_recipe(data, url):\n",
    "    return {\n",
    "        \"Title\": data.get(\"name\"),\n",
    "        \"Ingredients\": ', '.join(data.get(\"recipeIngredient\", [])),\n",
    "        \"Instructions\": (\n",
    "            ' '.join(\n",
    "                step.get(\"text\", \"\") for step in data.get(\"recipeInstructions\", [])\n",
    "                if isinstance(step, dict)\n",
    "            ) if isinstance(data.get(\"recipeInstructions\"), list)\n",
    "            else data.get(\"recipeInstructions\", \"\")\n",
    "        ),\n",
    "        \"URL\": url\n",
    "    }\n",
    "\n",
    "# Scrape each recipe\n",
    "recipes = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        data = extract_recipe_with_playwright(url)\n",
    "        if data:\n",
    "            recipe = parse_recipe(data, url)\n",
    "            recipes.append(recipe)\n",
    "            print(f\"âœ… Scraped: {url}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ No recipe found on: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error scraping {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe28eb57-5b07-43eb-9c16-ab8282a953bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš« No recipes were scraped.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Run the async function\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun(main())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/asyncio/runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "async def extract_recipe_async(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        try:\n",
    "            await page.goto(url, timeout=30000)\n",
    "            html = await page.content()\n",
    "        except Exception as e:\n",
    "            await browser.close()\n",
    "            print(f\"âŒ Error loading {url}: {e}\")\n",
    "            return None\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    scripts = soup.find_all('script', type='application/ld+json')\n",
    "\n",
    "    for script in scripts:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if item.get(\"@type\") == \"Recipe\":\n",
    "                        return parse_recipe(item, url)\n",
    "            elif data.get(\"@type\") == \"Recipe\":\n",
    "                return parse_recipe(data, url)\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(f\"âš ï¸ No recipe schema found on: {url}\")\n",
    "    return None\n",
    "\n",
    "def parse_recipe(data, url):\n",
    "    return {\n",
    "        \"Title\": data.get(\"name\"),\n",
    "        \"Ingredients\": ', '.join(data.get(\"recipeIngredient\", [])),\n",
    "        \"Instructions\": (\n",
    "            ' '.join(\n",
    "                step.get(\"text\", \"\") for step in data.get(\"recipeInstructions\", [])\n",
    "                if isinstance(step, dict)\n",
    "            ) if isinstance(data.get(\"recipeInstructions\"), list)\n",
    "            else data.get(\"recipeInstructions\", \"\")\n",
    "        ),\n",
    "        \"URL\": url\n",
    "    }\n",
    "\n",
    "async def main():\n",
    "    tasks = [extract_recipe_async(url) for url in urls]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    recipes = [r for r in results if r is not None]\n",
    "    \n",
    "    if recipes:\n",
    "        df = pd.DataFrame(recipes)\n",
    "        df.to_excel(\"async_playwright_recipes.xlsx\", index=False)\n",
    "        print(\"âœ… Recipes saved to async_playwright_recipes.xlsx\")\n",
    "    else:\n",
    "        print(\"ğŸš« No recipes were scraped.\")\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ea897-7c4e-4d33-9fa6-6699daab70f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
