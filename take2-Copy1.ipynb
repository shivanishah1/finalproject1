{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f47c0971-a7ea-436a-a192-43e1b3ad69ba",
   "metadata": {},
   "source": [
    "things to think about:\n",
    "\n",
    "- water: right now the plan is to do the weight of the guessed ingredients (so calculating them without the weight in the equations function) and subtract the actual weight from this weight. \n",
    "- further idea:\n",
    "based on how the product was cooked, figure out what percent of each ingredient was cooked off for the final weight. Then looking at the difference, it should be the water weight + how much cooked off from the other ingredients. This gives us the water weight. If there was no cooking invovled --> this process isn't needed. Just calculate the ingredients and subtract the given weight from that weight to get the water weight.\n",
    "QUESTIONS:\n",
    "- write a function that looks at how many minimums the system of equations has. Isn't this on the simpler side? It's a matrix, right?\n",
    "        - how do we reduce the number of minima? \n",
    "             - include calories in the initial calculation? or include calories in initial initialization. How does calories work when baking? Does the item get to be less calories when baked? No, stays the same while weight decreases --> meaning we can use calories to measure as well!!!!!!!\n",
    "             - what else...\n",
    "- could we do something with the information of knowing how many possibilities there are? like the number of ingredients and other properties (how nutritional dense they are (how many are heavy), how many significant nnutrient facts does the product have, etc.). With this knowledge, we couldd set a confidence score. \n",
    "\n",
    "SO IT SEEMS LIKE FOR TATES ITS NOT UNBLEACHED ENRICHED FLOUR BUT INSTEAD UNBLEACHED FLOUR. THIS HAS BEEN ADDING TO THE PROTEIN CATEGORY WAYYYY TOO MUCH. this makes sense..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2250611-102e-43d8-9848-71a66711385d",
   "metadata": {},
   "source": [
    "A bunch of libraries to import. The first four are standard regular ones that I've worked with before. The next few are ones I'm getting to know as the project continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3ac28d-26f6-4ac2-9a8e-81667212816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 18:10:28.787632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "#from bayes_opt import BayesianOptimization\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from functools import partial\n",
    "import requests #for the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79f50e5-eacf-42ba-95db-bd7ea5f0ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #for the API"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb7920bd-cd06-436a-8ad3-7d358c02b9e7",
   "metadata": {},
   "source": [
    "creating the overall dictionary: the one representing the nutrition label facts. I will copy this dictionary for any ingredient I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6751668-92a8-4761-9c07-0cf9432fbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_dict = {}\n",
    "\n",
    "overall_dict[\"Fat\"] = 0\n",
    "overall_dict[\"Saturated fatty acids\"] = 0\n",
    "overall_dict[\"Fatty acids, total trans\"] = 0\n",
    "overall_dict[\"Cholesterol\"] = 0\n",
    "overall_dict[\"Sodium\"] = 0\n",
    "overall_dict[\"Carbohydrate\"] = 0\n",
    "overall_dict[\"Fiber\"] = 0\n",
    "overall_dict[\"Sugars\"] = 0\n",
    "overall_dict[\"Protein\"] = 0\n",
    "overall_dict[\"Calcium\"] = 0\n",
    "overall_dict[\"Iron\"] = 0\n",
    "overall_dict[\"Potassium\"] = 0\n",
    "overall_dict[\"Vitamin D\"] = 0\n",
    "overall_dict[\"Weight\"] = 0\n",
    "overall_dict[\"Calories\"] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab94ec58-ef19-4883-98a6-c882f91c7095",
   "metadata": {},
   "source": [
    "nutritional information for tates cookies per serving (28 grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f647bf9-c62b-481b-a9f1-e694206ab906",
   "metadata": {},
   "outputs": [],
   "source": [
    "tates_dict = overall_dict.copy()\n",
    "tates_dict[\"Fat\"] = 7\n",
    "tates_dict[\"Saturated fatty acids\"] = 4.5\n",
    "tates_dict[\"Fatty acids, total trans\"] = 0\n",
    "tates_dict[\"Cholesterol\"] = .025\n",
    "tates_dict[\"Sodium\"] = .16\n",
    "tates_dict[\"Carbohydrate\"] = 18\n",
    "tates_dict[\"Fiber\"] = .8\n",
    "tates_dict[\"Sugars\"] = 12\n",
    "tates_dict[\"Protein\"] = 2\n",
    "tates_dict[\"Calcium\"] = .0000001\n",
    "tates_dict[\"Iron\"] = .0009\n",
    "tates_dict[\"Potassium\"] = .01\n",
    "tates_dict[\"Vitamin D\"] = .05\n",
    "tates_dict[\"Weight\"] = 28\n",
    "tates_dict[\"Calories\"] = 140"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9358b96-8d68-4b37-af78-721711632612",
   "metadata": {},
   "source": [
    "dictionary for vanilla –– what does sugars entail? is there a way to include alcohol content through the sugars number? or \n",
    "is it better to do it through calories? Because if it was in sugars, then we could calculate the calories by multiplication (of 9)?\n",
    "the two below are dictionaries with the Alex-given more specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb7f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_vanilla_dict = overall_dict.copy()\n",
    "#calories per 10 grams: 27 \n",
    "natural_vanilla_dict[\"Fat\"] = 0\n",
    "natural_vanilla_dict[\"Saturated fatty acids\"] = 0\n",
    "natural_vanilla_dict[\"Fatty acids, total trans\"] = 0\n",
    "natural_vanilla_dict[\"Cholesterol\"] = 0\n",
    "natural_vanilla_dict[\"Sodium\"] = .04 / 1000\n",
    "natural_vanilla_dict[\"Carbohydrate\"] = 1.2\n",
    "natural_vanilla_dict[\"Fiber\"] = 0\n",
    "natural_vanilla_dict[\"Sugars\"] = 0\n",
    "natural_vanilla_dict[\"Protein\"] = 0\n",
    "natural_vanilla_dict[\"Calcium\"] = 0\n",
    "natural_vanilla_dict[\"Iron\"] = 0\n",
    "natural_vanilla_dict[\"Potassium\"] = 0\n",
    "natural_vanilla_dict[\"Vitamin D\"] = 0\n",
    "natural_vanilla_dict[\"Weight\"] = 10\n",
    "natural_vanilla_dict[\"Calories\"] = 27\n",
    "\n",
    "baking_soda_alex_dict = overall_dict.copy()\n",
    "#calories per 10 grams: 0\n",
    "baking_soda_alex_dict[\"Fat\"] = 0\n",
    "baking_soda_alex_dict[\"Saturated fatty acids\"] = 0\n",
    "baking_soda_alex_dict[\"Fatty acids, total trans\"] = 0\n",
    "baking_soda_alex_dict[\"Cholesterol\"] = 0\n",
    "baking_soda_alex_dict[\"Sodium\"] = 2.736\n",
    "baking_soda_alex_dict[\"Carbohydrate\"] = 0\n",
    "baking_soda_alex_dict[\"Fiber\"] = 0\n",
    "baking_soda_alex_dict[\"Sugars\"] = 0\n",
    "baking_soda_alex_dict[\"Protein\"] = 0\n",
    "baking_soda_alex_dict[\"Calcium\"] = 0\n",
    "baking_soda_alex_dict[\"Iron\"] = 0\n",
    "baking_soda_alex_dict[\"Potassium\"] = 0\n",
    "baking_soda_alex_dict[\"Vitamin D\"] = 0\n",
    "baking_soda_alex_dict[\"Weight\"] = 10\n",
    "baking_soda_alex_dict[\"Calories\"] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16d5f14f-c12e-4499-97d7-42520c4d28ff",
   "metadata": {},
   "source": [
    "reading in the information downloaded in csv format by nutritionvalue.org"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5d6bcc1-60c9-4f06-83cb-1a3bf36b2014",
   "metadata": {},
   "source": [
    "TIME TO INCLUDE AN API???: https://fdc.nal.usda.gov/api-guide. requested key : pJdDYwC0EdXMpR18wMXFqjMxO4zYGCuTOfNfCSH4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "841c36e8-86ca-4f2f-91a3-7292ca460459",
   "metadata": {},
   "source": [
    "how to know which type of food? foundational? Now need to create a dictionary that stores which fdcID 's we are using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39587d4-cd7d-4b74-bcb4-818c77cfe828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a226568d-d71a-46a0-8392-daa8df875731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing results from Foundation Foods:\n",
      "Name: Flour, pastry, unenriched, unbleached\n",
      "FDC ID: 1104913\n",
      "------------------------------\n",
      "Name: Flour, bread, white, enriched, unbleached\n",
      "FDC ID: 790146\n",
      "------------------------------\n",
      "Name: Flour, wheat, all-purpose, enriched, unbleached\n",
      "FDC ID: 789951\n",
      "------------------------------\n",
      "Name: Flour, wheat, all-purpose, unenriched, unbleached\n",
      "FDC ID: 790018\n",
      "------------------------------\n",
      "Name: Flour, 00\n",
      "FDC ID: 2003586\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "api_key = \"pJdDYwC0EdXMpR18wMXFqjMxO4zYGCuTOfNfCSH4\"\n",
    "query = \"unbleached flour\"\n",
    "url = f\"https://api.nal.usda.gov/fdc/v1/foods/search?query={query}&dataType=Foundation&api_key={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "if data['foods']:\n",
    "    print(\"Showing results from Foundation Foods:\")\n",
    "else:\n",
    "    print(\"No Foundation Foods found. Searching Branded Foods...\")\n",
    "    url = f\"https://api.nal.usda.gov/fdc/v1/foods/search?query={query}&dataType=Branded&api_key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "# Print the first 5 results\n",
    "for food in data['foods'][:5]:\n",
    "    print(f\"Name: {food['description']}\")\n",
    "    print(f\"FDC ID: {food['fdcId']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1284dd2-f7c6-4dcf-bbe9-b74247e2c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_dict1 = {}\n",
    "\n",
    "overall_dict1[\"Total lipid (fat)\"] = 0\n",
    "overall_dict1[\"Saturated fatty acids\"] = 0\n",
    "overall_dict1[\"Fatty acids, total trans\"] = 0\n",
    "overall_dict1[\"Cholesterol\"] = 0\n",
    "overall_dict1[\"Sodium, Na\"] = 0\n",
    "overall_dict1[\"Carbohydrate\"] = 0\n",
    "overall_dict1[\"Fiber\"] = 0\n",
    "overall_dict1[\"Sugars\"] = 0\n",
    "overall_dict1[\"Protein\"] = 0\n",
    "overall_dict1[\"Calcium, Ca\"] = 0\n",
    "overall_dict1[\"Iron, Fe\"] = 0\n",
    "overall_dict1[\"Potassium, K\"] = 0\n",
    "overall_dict1[\"Vitamin D\"] = 0\n",
    "overall_dict1[\"Water\"] = 0\n",
    "overall_dict1[\"Weight\"] = 0\n",
    "overall_dict1['Energy\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a02125c-bdf4-422b-9a0f-d37e870182a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proximates\n",
      "NA g\n",
      "Water\n",
      "11.9 g\n",
      "Energy\n",
      "358.0 kcal\n",
      "Energy\n",
      "1500.0 kJ\n",
      "Nitrogen\n",
      "1.4 g\n",
      "Protein\n",
      "8.75 g\n",
      "Total lipid (fat)\n",
      "1.64 g\n",
      "Ash\n",
      "0.53 g\n",
      "Carbohydrates\n",
      "NA g\n",
      "Carbohydrate, by difference\n",
      "77.2 g\n",
      "Minerals\n",
      "NA mg\n",
      "Calcium, Ca\n",
      "17.0 mg\n",
      "Iron, Fe\n",
      "0.87 mg\n",
      "Magnesium, Mg\n",
      "22.4 mg\n",
      "Phosphorus, P\n",
      "102.0 mg\n",
      "Potassium, K\n",
      "142.0 mg\n",
      "Sodium, Na\n",
      "1.0 mg\n",
      "Zinc, Zn\n",
      "0.74 mg\n",
      "Copper, Cu\n",
      "0.159 mg\n",
      "Manganese, Mn\n",
      "0.635 mg\n",
      "Selenium, Se\n",
      "5.0 µg\n",
      "Molybdenum, Mo\n",
      "59.1 µg\n",
      "Vitamins and Other Components\n",
      "NA g\n",
      "Thiamin\n",
      "0.146 mg\n",
      "Riboflavin\n",
      "0.0 mg\n",
      "Niacin\n",
      "0.988 mg\n",
      "Vitamin B-6\n",
      "0.065 mg\n"
     ]
    }
   ],
   "source": [
    "fdc_id = data['foods'][0]['fdcId']\n",
    "nutrition_url = f\"https://api.nal.usda.gov/fdc/v1/food/{fdc_id}?api_key={api_key}\"\n",
    "\n",
    "response = requests.get(nutrition_url)\n",
    "nutrition_data = response.json()\n",
    "\n",
    "# Print nutrient details\n",
    "for nutrient in nutrition_data['foodNutrients']:\n",
    "    print(f\"{nutrient['nutrient']['name']}\")\n",
    "    amount = nutrient.get(\"amount\",\"NA\")\n",
    "    unit = nutrient.get(\"nutrient\", {}).get(\"unitName\", \"\")\n",
    "    print(amount,unit)\n",
    "    all_purpy_dict = oerall_dict1.copy()\n",
    "    for key in all_purpy_dict:\n",
    "        if key in nutrition_data['foodNutrients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "315b68bd-a761-4dea-a440-5830a52753b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_dict = overall_dict.copy()\n",
    "\n",
    "def reader(file,ingredient_dict):\n",
    "    with open(file, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for i,row in enumerate(csvreader):\n",
    "            if len(row) > 0 and row[0] in ingredient_dict:\n",
    "                if row[2] == \"mg\":\n",
    "                    ingredient_dict[row[0]] = (float(row[1]) / 1000)\n",
    "                elif row[2] == \"mcg\":\n",
    "                    ingredient_dict[row[0]] = (float(row[1]) / 1000000)\n",
    "                else: #grams\n",
    "                    ingredient_dict[row[0]] = (float(row[1]))\n",
    "            if i == 4:\n",
    "                index = row.index(\"g\")\n",
    "                ingredient_dict[\"Weight\"] = float(row[index-1])\n",
    "    return ingredient_dict"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12c2432f-e5bd-4b31-a1b0-c0f55e28ad09",
   "metadata": {},
   "source": [
    "the various ingredients in Tate's. Transforming the online nutrritional information of each ingredient into individual dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1058471-cc4e-4910-a56e-d9a8c3fd7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_sweet_chocolate_file = \"semisweet_chocolate_chips_by_raleys.csv\"\n",
    "semi_sweet_chocolate_dict = overall_dict.copy()\n",
    "reader(semi_sweet_chocolate_file,semi_sweet_chocolate_dict)\n",
    "\n",
    "\n",
    "#need to change this.\n",
    "unbleached_flour_file = \"flour_unbleached_enriched_allpurpose_wheat.csv\"\n",
    "unbleached_flour_dict = overall_dict.copy()\n",
    "reader(unbleached_flour_file,unbleached_flour_dict)\n",
    "unbleached_flour_dict[\"Protein\"] = .875\n",
    "\n",
    "salted_butter_file = \"butter_salted.csv\"\n",
    "salted_butter_dict = overall_dict.copy()\n",
    "reader(salted_butter_file,salted_butter_dict)\n",
    "\n",
    "cane_sugar_file = \"granulated_pure_cane_sugar.csv\"\n",
    "cane_sugar_dict = overall_dict.copy()\n",
    "reader(cane_sugar_file,cane_sugar_dict)\n",
    "\n",
    "brown_cane_sugar_file = \"brown_sugar_cane_by_frusecha.csv\"\n",
    "brown_cane_sugar_dict = overall_dict.copy()\n",
    "reader(brown_cane_sugar_file,brown_cane_sugar_dict)\n",
    "\n",
    "eggs_file = \"egg_fresh_raw_whole.csv\"\n",
    "eggs_dict = overall_dict.copy()\n",
    "reader(eggs_file,eggs_dict)\n",
    "\n",
    "baking_soda_file = \"leavening_agents_baking_soda.csv\"\n",
    "baking_soda_dict = overall_dict.copy()\n",
    "reader(baking_soda_file,baking_soda_dict)\n",
    "\n",
    "salt_file = \"salt_table.csv\"\n",
    "salt_dict = overall_dict.copy()\n",
    "reader(salt_file,salt_dict)\n",
    "\n",
    "natural_vanilla_flavor_file = \"vanilla_flavoring_syrup_by_r_torre__coinc.csv\"\n",
    "natural_vanilla_flavor_dict = overall_dict.copy()\n",
    "reader(natural_vanilla_flavor_file,natural_vanilla_flavor_dict)\n",
    "\n",
    "vanilla_extract_file_1 = \"vanilla_extract.csv\"\n",
    "vanilla_extract_dict_1 = overall_dict.copy()\n",
    "reader(vanilla_extract_file_1,vanilla_extract_dict_1)\n",
    "\n",
    "dictionary_list = [semi_sweet_chocolate_dict,unbleached_flour_dict,salted_butter_dict,cane_sugar_dict,brown_cane_sugar_dict,eggs_dict,baking_soda_alex_dict,salt_dict,natural_vanilla_dict]\n",
    "heavy_dictionary_list = [semi_sweet_chocolate_dict,unbleached_flour_dict,salted_butter_dict,cane_sugar_dict,brown_cane_sugar_dict,eggs_dict]\n",
    "key_list = [\"Fat\",\"Saturated fatty acids\",\"Fatty acids, total trans\",\"Cholesterol\",\"Sodium\",\"Carbohydrate\",\"Fiber\",\"Sugars\",\"Protein\",\"Calcium\",\"Iron\",\"Potassium\",\"Vitamin D\",\"Weight\",\"Calories\"]\n",
    "key_list_wo_weight = [\"Fat\",\"Saturated fatty acids\",\"Fatty acids, total trans\",\"Cholesterol\",\"Sodium\",\"Carbohydrate\",\"Fiber\",\"Sugars\",\"Protein\",\"Calcium\",\"Iron\",\"Potassium\",\"Vitamin D\",\"Calories\"]\n",
    "key_list_wo_sodium_and_weight = [\"Fat\",\"Saturated fatty acids\",\"Fatty acids, total trans\",\"Cholesterol\",\"Carbohydrate\",\"Fiber\",\"Sugars\",\"Protein\",\"Calcium\",\"Iron\",\"Potassium\",\"Vitamin D\",\"Calories\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c532bde4-9e1d-47a4-8af8-0f076106a939",
   "metadata": {},
   "source": [
    "just generates a list with length of inputted dictionary_list (so has a length of number of ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "635be9c7-5981-4067-a050-919369cfd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_x_list(number,dictionary_list):\n",
    "    x_list = []\n",
    "    for variable in range(0, len(dictionary_list)):\n",
    "        x_list.append(number)\n",
    "    return x_list\n",
    "\n",
    "x_list = building_x_list(1,dictionary_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e5ec123-1e93-4f6d-b2a0-318b24540711",
   "metadata": {},
   "source": [
    "function that evaluates the guesses and compares them with the goal values (nutritonal info for target product). Divided by 10 when evaluating because each dictionary is out of 10 grams. So if I had 20 grams of eggs, I'd multiply each nutritional fact by 2 (as each fact is per 10 grams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71dd96b6-b1f4-4153-aca4-22ec74dd5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equations(dictionary_list,x_list,desired_dict,key_list):\n",
    "    list_of_equations = []\n",
    "    for key in key_list:\n",
    "        equation = 0\n",
    "        for i,dictionary in enumerate(dictionary_list):\n",
    "            equation = equation + (dictionary[key] * (x_list[i] / 10))\n",
    "        list_of_equations.append(equation-(desired_dict[key]))\n",
    "    return list_of_equations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc5e9518-9e92-4271-8cd1-46702795ce2e",
   "metadata": {},
   "source": [
    "initializes variables, choosing random numbers between 0 and the largest weight possible. The variables all together should add up to the final weight. So first, I create these values. Then sort them in descending order. Then divide by a normalizing factor to ensure they add up to the final weight. This factor is just the weight / sum of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00a9a79a-8b3c-45bf-af72-8f3462bd4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_variables_tf(n,desired_dict):\n",
    "    largest_weight = desired_dict[\"Weight\"]\n",
    "\n",
    "    initial_values = tf.random.uniform(shape=(n,), minval=0.0, maxval=largest_weight)\n",
    "    initial_values = tf.sort(initial_values, direction='DESCENDING')\n",
    "\n",
    "    initial_values = (initial_values * largest_weight) / tf.reduce_sum(initial_values)\n",
    "    \n",
    "    return tf.Variable(initial_values, trainable=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87edc75d-c7a6-477c-8fc7-54782abd385f",
   "metadata": {},
   "source": [
    "this is the loss function. need to update this to provide more info.\n",
    "what can I add?\n",
    "- parameters: all variables are > 0, in descending order, equations --> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59c2c77b-f00b-4971-a70b-0eee2acc166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_tf(variables_tf,predicted):\n",
    "\n",
    "    error = tf.reduce_mean(tf.square(predicted))\n",
    "    error = error +  5*(tf.reduce_sum(tf.square(tf.minimum(variables_tf, 0))))\n",
    "    \n",
    "    variables_tf_array = variables_tf.numpy()\n",
    "    for i in range(0,len(variables_tf_array)-2):\n",
    "        if variables_tf_array[i] - variables_tf_array[i + 1] < 0:\n",
    "            error = error + ((variables_tf_array[i] - variables_tf_array[i + 1]) ** 2)\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2493d7d6-f1db-430c-91f9-12946c9bc1e8",
   "metadata": {},
   "source": [
    "now this is the training loop."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f27ebe8-c7b6-4a0c-b329-53704c901e6e",
   "metadata": {},
   "source": [
    "In this tates example, this issue comes up: chocolate chips overpowers the whole recipe.\n",
    "How do we change this:\n",
    "- restrict initial input of chocolate chips?\n",
    "- maintain a ratio?\n",
    "- standard deviation? \n",
    "\n",
    "\n",
    "could we change the learning rate as we go on? like the first few have a certain learning rate then we change it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25e92179-5f5f-41c8-bb75-88c859639ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_tf(dictionary_list,target_dict,key_list):  \n",
    "    #number of ingredients\n",
    "    n = len(dictionary_list)\n",
    "    #first guess\n",
    "    variables_tf = initialize_variables_tf(n,target_dict)\n",
    "    print(variables_tf)\n",
    "    print(tf.reduce_sum(variables_tf).numpy())\n",
    "    #the optimizer chosen: stochastic gradient descent. WHAT IF WE TRIED batch gradient descent here?\n",
    "    #pros/cons: https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/\n",
    "    #whats the math behind this?\n",
    "        #updates each variable. the variable is updated to be its value - \n",
    "        #the learning rate * the gradient of how much that variable affects the loss.\n",
    "        #THIS PART IS DIFFERENT FROM MY PREVIOUS METHOD MATHEMATICALLY, but does the same thing\n",
    "        #SHOW tests for learning rates. date the comments.\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=.01)\n",
    "\n",
    "# Training loop\n",
    "    #redefines the variable 600 times. what number is OPTIMAL?\n",
    "    for epoch in range(600):\n",
    "\n",
    "        #the \"with\" allows for automatic differentiation. this means the with part locks in the operations. but here no gradients are calculated.\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            predictions1 = equations(dictionary_list,variables_tf,target_dict,key_list)\n",
    "            predictions = tf.convert_to_tensor(predictions1)\n",
    "            loss = loss_tf(variables_tf,predictions)\n",
    "\n",
    "        #gradients are computed here.\n",
    "        gradients = tape.gradient(loss, [variables_tf])\n",
    "\n",
    "        #apply these gradients, perform sgd on our variables (which are being updated) \n",
    "        optimizer.apply_gradients(zip(gradients, [variables_tf]))\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print((tf.reduce_sum(variables_tf).numpy()))\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}, Variables: {variables_tf.numpy()}\")\n",
    "\n",
    "    return variables_tf.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b66f2f9e-04b7-4292-bbbb-7887359bc78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6,) dtype=float32, numpy=\n",
      "array([6.581594 , 6.5568275, 5.165269 , 4.123789 , 3.200676 , 2.3718445],\n",
      "      dtype=float32)>\n",
      "28.000002\n",
      "28.591248\n",
      "Epoch 0, Loss: 21.5745, Variables: [6.6963167 6.644034  5.3370123 4.2186294 3.2900026 2.4052498]\n",
      "31.435423\n",
      "Epoch 20, Loss: 0.9200, Variables: [7.260479  7.0548596 6.208421  4.6763024 3.7162943 2.5190685]\n",
      "31.442177\n",
      "Epoch 40, Loss: 0.8734, Variables: [7.27774   7.0447955 6.2671804 4.6802044 3.7136467 2.45861  ]\n",
      "31.375498\n",
      "Epoch 60, Loss: 0.8436, Variables: [7.280595  7.0242476 6.3019147 4.6733437 3.7007856 2.3946135]\n",
      "31.30925\n",
      "Epoch 80, Loss: 0.8156, Variables: [7.2832737 7.0041666 6.333899  4.6674647 3.6888213 2.3316233]\n",
      "31.245272\n",
      "Epoch 100, Loss: 0.7893, Variables: [7.2861395 6.9848137 6.3638535 4.6627707 3.677958  2.2697349]\n",
      "31.183483\n",
      "Epoch 120, Loss: 0.7645, Variables: [7.289182  6.9661603 6.3919353 4.659166  3.6681144 2.2089248]\n",
      "31.123768\n",
      "Epoch 140, Loss: 0.7410, Variables: [7.292383 6.948177 6.418278 4.656553 3.659209 2.149168]\n",
      "31.066011\n",
      "Epoch 160, Loss: 0.7188, Variables: [7.295723  6.930835  6.443003  4.654843  3.6511672 2.0904403]\n",
      "31.010107\n",
      "Epoch 180, Loss: 0.6977, Variables: [7.2991877 6.9141064 6.466225  4.653952  3.643917  2.032718 ]\n",
      "30.955957\n",
      "Epoch 200, Loss: 0.6776, Variables: [7.302762  6.8979654 6.4880486 4.653807  3.6373956 1.975979 ]\n",
      "30.903473\n",
      "Epoch 220, Loss: 0.6584, Variables: [7.306432  6.8823876 6.5085707 4.6543374 3.6315427 1.920202 ]\n",
      "30.852568\n",
      "Epoch 240, Loss: 0.6401, Variables: [7.3101864 6.867351  6.527882  4.655479  3.6263027 1.8653656]\n",
      "30.803158\n",
      "Epoch 260, Loss: 0.6227, Variables: [7.3140135 6.852833  6.5460668 4.6571717 3.6216247 1.8114498]\n",
      "30.755177\n",
      "Epoch 280, Loss: 0.6059, Variables: [7.317902  6.8388143 6.5632005 4.6593623 3.6174624 1.7584352]\n",
      "30.708548\n",
      "Epoch 300, Loss: 0.5899, Variables: [7.321844  6.8252754 6.5793567 4.661998  3.613772  1.7063029]\n",
      "30.663212\n",
      "Epoch 320, Loss: 0.5746, Variables: [7.3258286 6.812198  6.5946016 4.665035  3.6105134 1.6550349]\n",
      "30.619104\n",
      "Epoch 340, Loss: 0.5598, Variables: [7.3298492 6.799566  6.6089964 4.6684294 3.6076503 1.6046128]\n",
      "30.576168\n",
      "Epoch 360, Loss: 0.5456, Variables: [7.3338985 6.787362  6.622598  4.672142  3.605148  1.55502  ]\n",
      "30.534353\n",
      "Epoch 380, Loss: 0.5320, Variables: [7.33797   6.77557   6.6354613 4.676138  3.6029754 1.5062395]\n",
      "30.493612\n",
      "Epoch 400, Loss: 0.5189, Variables: [7.342057  6.7641788 6.647632  4.6803846 3.6011026 1.4582558]\n",
      "30.45389\n",
      "Epoch 420, Loss: 0.5063, Variables: [7.346154  6.753173  6.659157  4.6848497 3.5995045 1.4110528]\n",
      "30.41515\n",
      "Epoch 440, Loss: 0.4941, Variables: [7.350255  6.742536  6.6700797 4.6895065 3.598156  1.364616 ]\n",
      "30.37735\n",
      "Epoch 460, Loss: 0.4824, Variables: [7.354356  6.7322607 6.6804395 4.6943307 3.597034  1.3189303]\n",
      "30.340454\n",
      "Epoch 480, Loss: 0.4711, Variables: [7.3584547 6.722333  6.690271  4.699296  3.5961175 1.273981 ]\n",
      "30.30442\n",
      "Epoch 500, Loss: 0.4602, Variables: [7.3625455 6.712742  6.6996083 4.7043843 3.5953872 1.2297546]\n",
      "30.269222\n",
      "Epoch 520, Loss: 0.4497, Variables: [7.366625  6.7034774 6.708484  4.7095723 3.5948262 1.1862371]\n",
      "30.234821\n",
      "Epoch 540, Loss: 0.4400, Variables: [7.3706903 6.6945286 6.7169256 4.7148438 3.594417  1.1434152]\n",
      "30.20119\n",
      "Epoch 560, Loss: 0.4311, Variables: [7.374739  6.685887  6.7249603 4.720183  3.5941453 1.1012765]\n",
      "30.1683\n",
      "Epoch 580, Loss: 0.4232, Variables: [7.3787675 6.6775413 6.732614  4.7255735 3.5939963 1.0598079]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.382574558258057,\n",
       " 6.669881820678711,\n",
       " 6.739553451538086,\n",
       " 4.730730056762695,\n",
       " 3.5939579010009766,\n",
       " 1.0210227966308594]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_tf(heavy_dictionary_list,tates_dict,key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3a50173-2bb5-4ebf-aa0d-d56d5cf6fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6,) dtype=float32, numpy=\n",
      "array([6.581594 , 6.5568275, 5.165269 , 4.123789 , 3.200676 , 2.3718445],\n",
      "      dtype=float32)>\n",
      "28.000002\n",
      "28.591248\n",
      "Epoch 0, Loss: 21.5745, Variables: [6.6963167 6.644034  5.3370123 4.2186294 3.2900026 2.4052498]\n",
      "31.435423\n",
      "Epoch 20, Loss: 0.9200, Variables: [7.260479  7.0548596 6.208421  4.6763024 3.7162943 2.5190685]\n",
      "31.442177\n",
      "Epoch 40, Loss: 0.8734, Variables: [7.27774   7.0447955 6.2671804 4.6802044 3.7136467 2.45861  ]\n",
      "31.375498\n",
      "Epoch 60, Loss: 0.8436, Variables: [7.280595  7.0242476 6.3019147 4.6733437 3.7007856 2.3946135]\n",
      "31.30925\n",
      "Epoch 80, Loss: 0.8156, Variables: [7.2832737 7.0041666 6.333899  4.6674647 3.6888213 2.3316233]\n",
      "31.245272\n",
      "Epoch 100, Loss: 0.7893, Variables: [7.2861395 6.9848137 6.3638535 4.6627707 3.677958  2.2697349]\n",
      "31.183483\n",
      "Epoch 120, Loss: 0.7645, Variables: [7.289182  6.9661603 6.3919353 4.659166  3.6681144 2.2089248]\n",
      "31.123768\n",
      "Epoch 140, Loss: 0.7410, Variables: [7.292383 6.948177 6.418278 4.656553 3.659209 2.149168]\n",
      "31.066011\n",
      "Epoch 160, Loss: 0.7188, Variables: [7.295723  6.930835  6.443003  4.654843  3.6511672 2.0904403]\n",
      "31.010107\n",
      "Epoch 180, Loss: 0.6977, Variables: [7.2991877 6.9141064 6.466225  4.653952  3.643917  2.032718 ]\n",
      "30.955957\n",
      "Epoch 200, Loss: 0.6776, Variables: [7.302762  6.8979654 6.4880486 4.653807  3.6373956 1.975979 ]\n",
      "30.903473\n",
      "Epoch 220, Loss: 0.6584, Variables: [7.306432  6.8823876 6.5085707 4.6543374 3.6315427 1.920202 ]\n",
      "30.852568\n",
      "Epoch 240, Loss: 0.6401, Variables: [7.3101864 6.867351  6.527882  4.655479  3.6263027 1.8653656]\n",
      "30.803158\n",
      "Epoch 260, Loss: 0.6227, Variables: [7.3140135 6.852833  6.5460668 4.6571717 3.6216247 1.8114498]\n",
      "30.755177\n",
      "Epoch 280, Loss: 0.6059, Variables: [7.317902  6.8388143 6.5632005 4.6593623 3.6174624 1.7584352]\n",
      "30.708548\n",
      "Epoch 300, Loss: 0.5899, Variables: [7.321844  6.8252754 6.5793567 4.661998  3.613772  1.7063029]\n",
      "30.663212\n",
      "Epoch 320, Loss: 0.5746, Variables: [7.3258286 6.812198  6.5946016 4.665035  3.6105134 1.6550349]\n",
      "30.619104\n",
      "Epoch 340, Loss: 0.5598, Variables: [7.3298492 6.799566  6.6089964 4.6684294 3.6076503 1.6046128]\n",
      "30.576168\n",
      "Epoch 360, Loss: 0.5456, Variables: [7.3338985 6.787362  6.622598  4.672142  3.605148  1.55502  ]\n",
      "30.534353\n",
      "Epoch 380, Loss: 0.5320, Variables: [7.33797   6.77557   6.6354613 4.676138  3.6029754 1.5062395]\n",
      "30.493612\n",
      "Epoch 400, Loss: 0.5189, Variables: [7.342057  6.7641788 6.647632  4.6803846 3.6011026 1.4582558]\n",
      "30.45389\n",
      "Epoch 420, Loss: 0.5063, Variables: [7.346154  6.753173  6.659157  4.6848497 3.5995045 1.4110528]\n",
      "30.41515\n",
      "Epoch 440, Loss: 0.4941, Variables: [7.350255  6.742536  6.6700797 4.6895065 3.598156  1.364616 ]\n",
      "30.37735\n",
      "Epoch 460, Loss: 0.4824, Variables: [7.354356  6.7322607 6.6804395 4.6943307 3.597034  1.3189303]\n",
      "30.340454\n",
      "Epoch 480, Loss: 0.4711, Variables: [7.3584547 6.722333  6.690271  4.699296  3.5961175 1.273981 ]\n",
      "30.30442\n",
      "Epoch 500, Loss: 0.4602, Variables: [7.3625455 6.712742  6.6996083 4.7043843 3.5953872 1.2297546]\n",
      "30.269222\n",
      "Epoch 520, Loss: 0.4497, Variables: [7.366625  6.7034774 6.708484  4.7095723 3.5948262 1.1862371]\n",
      "30.234821\n",
      "Epoch 540, Loss: 0.4400, Variables: [7.3706903 6.6945286 6.7169256 4.7148438 3.594417  1.1434152]\n",
      "30.20119\n",
      "Epoch 560, Loss: 0.4311, Variables: [7.374739  6.685887  6.7249603 4.720183  3.5941453 1.1012765]\n",
      "30.1683\n",
      "Epoch 580, Loss: 0.4232, Variables: [7.3787675 6.6775413 6.732614  4.7255735 3.5939963 1.0598079]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.382574558258057,\n",
       " 6.669881820678711,\n",
       " 6.739553451538086,\n",
       " 4.730730056762695,\n",
       " 3.5939579010009766,\n",
       " 1.0210227966308594]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_tf(heavy_dictionary_list,tates_dict,key_list_wo_sodium_or_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e24c3b4-8f83-4c32-abc7-fa336025c93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6,) dtype=float32, numpy=\n",
      "array([7.569378  , 6.2830787 , 6.244275  , 4.2315645 , 3.244995  ,\n",
      "       0.42670557], dtype=float32)>\n",
      "27.999998\n",
      "28.284037\n",
      "Epoch 0, Loss: 5.0285, Variables: [7.624502  6.3252068 6.3255267 4.2777534 3.288414  0.442633 ]\n",
      "29.661541\n",
      "Epoch 20, Loss: 0.3267, Variables: [7.8978915  6.528777   6.723259   4.5114446  3.5046082  0.49556178]\n",
      "29.67849\n",
      "Epoch 40, Loss: 0.3209, Variables: [7.9090443  6.530411   6.7331247  4.5266175  3.514378   0.46491402]\n",
      "29.659147\n",
      "Epoch 60, Loss: 0.3164, Variables: [7.9130816 6.526765  6.7328205 4.5355897 3.5183911 0.4325016]\n",
      "29.63899\n",
      "Epoch 80, Loss: 0.3121, Variables: [7.916837  6.5231056 6.732438  4.544064  3.5220358 0.4005089]\n",
      "29.61897\n",
      "Epoch 100, Loss: 0.3080, Variables: [7.9205     6.5195737  6.7322283  4.5522118  3.5254688  0.36898747]\n",
      "29.599125\n",
      "Epoch 120, Loss: 0.3041, Variables: [7.924078  6.516173  6.732182  4.5600533 3.5287073 0.3379323]\n",
      "29.579466\n",
      "Epoch 140, Loss: 0.3004, Variables: [7.9275737 6.5129027 6.7322826 4.5676045 3.531764  0.3073365]\n",
      "29.559996\n",
      "Epoch 160, Loss: 0.2969, Variables: [7.930992   6.509762   6.7325163  4.57488    3.5346498  0.27719373]\n",
      "29.540722\n",
      "Epoch 180, Loss: 0.2937, Variables: [7.934336   6.5067496  6.732869   4.581894   3.5373755  0.24749713]\n",
      "29.521652\n",
      "Epoch 200, Loss: 0.2905, Variables: [7.9376087  6.503866   6.7333274  4.5886583  3.539949   0.21824019]\n",
      "29.50279\n",
      "Epoch 220, Loss: 0.2876, Variables: [7.940813   6.50111    6.73388    4.5951877  3.542382   0.18941657]\n",
      "29.48414\n",
      "Epoch 240, Loss: 0.2848, Variables: [7.943951   6.498479   6.7345176  4.6014905  3.5446813  0.16101973]\n",
      "29.465704\n",
      "Epoch 260, Loss: 0.2821, Variables: [7.9470243  6.495973   6.7352285  4.607579   3.546855   0.13304321]\n",
      "29.44749\n",
      "Epoch 280, Loss: 0.2795, Variables: [7.950037   6.4935904  6.736006   4.6134634  3.548911   0.10548101]\n",
      "29.429493\n",
      "Epoch 300, Loss: 0.2771, Variables: [7.95299    6.491329   6.736841   4.619152   3.5508547  0.07832629]\n",
      "29.411718\n",
      "Epoch 320, Loss: 0.2748, Variables: [7.9558854  6.4891887  6.7377257  4.6246533  3.5526934  0.05157327]\n",
      "29.39417\n",
      "Epoch 340, Loss: 0.2725, Variables: [7.9587274 6.4871674 6.738653  4.6299763 3.5544324 0.0252158]\n",
      "29.376848\n",
      "Epoch 360, Loss: 0.2704, Variables: [ 7.9615149e+00  6.4852624e+00  6.7396169e+00  4.6351285e+00\n",
      "  3.5560777e+00 -7.5233879e-04]\n",
      "29.371841\n",
      "Epoch 380, Loss: 0.2706, Variables: [ 7.963683    6.483009    6.7397866   4.6396456   3.5571802  -0.01146202]\n",
      "29.371964\n",
      "Epoch 400, Loss: 0.2715, Variables: [ 7.964979    6.4801345   6.7388277   4.64333     3.5575256  -0.01283279]\n",
      "29.372362\n",
      "Epoch 420, Loss: 0.2723, Variables: [ 7.966082    6.477139    6.7377453   4.646758    3.5576453  -0.01300825]\n",
      "29.37271\n",
      "Epoch 440, Loss: 0.2733, Variables: [ 7.9671383  6.474135   6.736744   4.650057   3.5576606 -0.0130259]\n",
      "29.37298\n",
      "Epoch 460, Loss: 0.2743, Variables: [ 7.96817     6.4711375   6.7358465   4.653251    3.557594   -0.01302241]\n",
      "29.373169\n",
      "Epoch 480, Loss: 0.2753, Variables: [ 7.9691815   6.4681506   6.7350497   4.65635     3.5574524  -0.01301601]\n",
      "29.373293\n",
      "Epoch 500, Loss: 0.2765, Variables: [ 7.970176    6.465178    6.734345    4.659361    3.557242   -0.01300902]\n",
      "29.373344\n",
      "Epoch 520, Loss: 0.2776, Variables: [ 7.9711523   6.462217    6.7337236   4.6622887   3.5569668  -0.01300195]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m running_tf(heavy_dictionary_list,tates_dict,key_list)\n",
      "Cell \u001b[0;32mIn[34], line 24\u001b[0m, in \u001b[0;36mrunning_tf\u001b[0;34m(dictionary_list, target_dict, key_list)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m600\u001b[39m):\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#the \"with\" allows for automatic differentiation. this means the with part locks in the operations. but here no gradients are calculated.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 24\u001b[0m         predictions1 \u001b[38;5;241m=\u001b[39m equations(dictionary_list,variables_tf,target_dict,key_list)\n\u001b[1;32m     25\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(predictions1)\n\u001b[1;32m     26\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_tf(variables_tf,predictions)\n",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m, in \u001b[0;36mequations\u001b[0;34m(dictionary_list, x_list, desired_dict, key_list)\u001b[0m\n\u001b[1;32m      4\u001b[0m     equation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,dictionary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dictionary_list):\n\u001b[0;32m----> 6\u001b[0m         equation \u001b[38;5;241m=\u001b[39m equation \u001b[38;5;241m+\u001b[39m (dictionary[key] \u001b[38;5;241m*\u001b[39m (x_list[i] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      7\u001b[0m     list_of_equations\u001b[38;5;241m.\u001b[39mappend(equation\u001b[38;5;241m-\u001b[39m(desired_dict[key]))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m list_of_equations\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/tensor_getitem_override.py:311\u001b[0m, in \u001b[0;36m_slice_helper_var\u001b[0;34m(var, slice_spec)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slice_helper_var\u001b[39m(var, slice_spec):\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a slice helper object given a variable.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m  This allows creating a sub-tensor from part of the current contents\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _slice_helper(var\u001b[38;5;241m.\u001b[39mvalue(), slice_spec, var)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/resource_variable_ops.py:629\u001b[0m, in \u001b[0;36mBaseResourceVariable.value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_value\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 629\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_variable_op()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/resource_variable_ops.py:798\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_variable_op\u001b[39m(\u001b[38;5;28mself\u001b[39m, no_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    787\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reads the value of the variable.\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m  If the variable is in copy-on-read mode and `no_copy` is True, the variable\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m    The value of the variable.\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m   variable_accessed(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    800\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_and_set_handle\u001b[39m(no_copy):\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/resource_variable_ops.py:330\u001b[0m, in \u001b[0;36mvariable_accessed\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvariable_accessed\u001b[39m(variable):\n\u001b[1;32m    329\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Records that `variable` was accessed for the tape and FuncGraph.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ops\u001b[38;5;241m.\u001b[39mget_default_graph(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatch_variable\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    331\u001b[0m     ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39mwatch_variable(variable)\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m variable\u001b[38;5;241m.\u001b[39mtrainable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "running_tf(heavy_dictionary_list,tates_dict,key_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "378f4182-74c6-464b-a416-72b18388c550",
   "metadata": {},
   "source": [
    "issue: there are way too many local minimum. How do I find the one I want? \n",
    "- I could present all my local minimum possibilities... how to pick the certain one?\n",
    "- do I calculate a bunch and choose the one with lowest loss? Do I calculate and then find the most recurring one? This one appears the most naturally?\n",
    "- how can I incorporate food science ideas? Maybe I can add relationships between the ingredients. so if a food has more than a few sugar-contributing ingredients, etc... what to do with that?\n",
    "- could I pass each guess through a \"viable food product\" type of function?\n",
    "    - processing-aware constraints, so there must be a specific flour to fat ratio for proper dough structure. or high sugar/fat ratio could cause spreading in cookies, etc. if something is very off the ratio?\n",
    "    - is there a way we could train the model with existing cookie recipes so that it can understand ratios? How do you even do this? once it understands ratios of other recipes, then when looking at all the possibilities it outputted, it can choose which one makes the most sense.\n",
    "    - put the guess through AI and have it say if its viable?\n",
    "    - make custom GPT for ratio detection from past recipes & online research & books\n",
    "    - maybe after each iteration, ask questions like\n",
    "        - does it have enough liquid? (ex. w the tates cookies, if it doesn't, then increase the eggs --> decrease the chocolate chips --> increase white/brown sugar). \n",
    "        - how would we know how much liquid it needs? maybe someone would input the recipe along with tags: - how dry, how crumbly, how many chocolate chips it seemingly has, etc. But isn't this similar to using the sensory analysis? Or isn't this just giving a helping hand. For example, if someone says this cookie has medium chocolate chips, then we restraint our chocolate chip guess to a specific ratio of the dough. \n",
    "            - what if someone inputted the nutritional label + possible dominating ingredients? That way if there are no dominating ingredients, we want the ratios of each one to follow some sort of pattern/curve. But if there is a dominating ingredient (ex. water in a water-based beverage) then we make an exception. so no dominating means if the product has multiple ingredients that contribute to the same nutrient, ensuring that one doesn't hog all of it (assuming these are all heavier ingredients)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "feb0d3d9-669c-4e06-b5ff-0ba239f61c94",
   "metadata": {},
   "source": [
    "after optimizing the heavy ingredients, add noise to the list to make room for lighter ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0ffb859c-6f46-47b1-8a9c-ee4a176d687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(optimized_heavy_ingredients):\n",
    "    optimized_w_noise = []\n",
    "    for i in range(0,len(optimized_heavy_ingredients)):\n",
    "        number = random.randint(1,2)\n",
    "        if number == 1:\n",
    "            optimized_w_noise.append(optimized_heavy_ingredients[i] - .1)\n",
    "        else:\n",
    "            optimized_w_noise.append(optimized_heavy_ingredients[i] + .1)\n",
    "    return optimized_w_noise"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b05c6534-ae69-4109-9939-edbb61b2f747",
   "metadata": {},
   "source": [
    "error function for combined heavier + lighter ingredients. This looks at how the \"equations\" function evaluates as well as the calories. WOULD IT BE SMARTER TO MOVE THE CALORIES PORTION? Also, look at the 100 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "26f76581-2589-43ce-bfc2-f214c32f3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function_lighter(lighter_ingredients,optimized_heavy_ingredients):\n",
    "\n",
    "    full_ingredients = np.concatenate((optimized_heavy_ingredients, lighter_ingredients))\n",
    "    equations_list = equations(dictionary_list,full_ingredients,tates_dict,key_list_wo_weight)\n",
    "    error = 0\n",
    "    for i, equation in enumerate(equations_list):\n",
    "        #if i == 4 or i == 5:\n",
    "        #    error += equation ** 2\n",
    "        #else:\n",
    "        error += equation\n",
    "\n",
    "    #gives the total carbs/fats/proteins as looks at the difference + targeted amount --> actual amount\n",
    "    carbs = equations_list[5] + tates_dict[\"Carbohydrate\"]\n",
    "    fats = equations_list[0] + tates_dict[\"Fat\"]\n",
    "    proteins = equations_list[8] + tates_dict[\"Protein\"]\n",
    "    calorie_count = (carbs * 4) + (fats * 9) + (proteins * 4)\n",
    "    #the 140 is tates specific\n",
    "    difference_in_calories = ((calorie_count) - 140) ** 2\n",
    "\n",
    "    #dividing by 100 here because otherwise the error is too high - leaves bayesian optimization to just choose lowest option for lightest ingredients\n",
    "    error += (difference_in_calories / 10)   \n",
    "    return abs(error)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0a5ee62-fb09-4543-b67c-74c139c8ee61",
   "metadata": {},
   "source": [
    "Bayesian occurance. Samples points + finds the points that contribute to the least error. \n",
    "\n",
    "There is so much to play around with here\n",
    "- the error function. this basically dictates everything. The bayesian function tries (# of points we tell it to) and finds the best match.  this best match is entirely decided by the error function. Here, the error function is error_function_lighter. What to include and not include in this function to make it the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b617dee6-966d-48db-a23b-b424a921f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function_bayesian(lighter_ingredients, optimized_heavy_ingredients):\n",
    "    return error_function_lighter(lighter_ingredients, optimized_heavy_ingredients)\n",
    "\n",
    "# Map lighter ingredients to their indices\n",
    "lighter_indices = [6, 7, 8]  # vanilla, baking soda, xanthan gum\n",
    "\n",
    "# Define the search space for Bayesian Optimization using the indices\n",
    "search_space = [\n",
    "    Real(0.00001, 1, name=f\"ingredient_{lighter_indices[0]}\"),  # baking_soda\n",
    "    Real(0.00001, 1, name=f\"ingredient_{lighter_indices[1]}\"),  # salt\n",
    "    Real(0.00001, 1, name=f\"ingredient_{lighter_indices[2]}\")   # vanilla\n",
    "]\n",
    "\n",
    "def bayesiann(optimized_heavy_ingredients):\n",
    "    print(\"Optimized heavy ingredients passed to bayesiann:\")\n",
    "    print(optimized_heavy_ingredients)\n",
    "\n",
    "    error_function_fixed = partial(error_function_lighter, optimized_heavy_ingredients=optimized_heavy_ingredients)\n",
    "    result = gp_minimize(\n",
    "        func=error_function_fixed,\n",
    "        dimensions=search_space,\n",
    "        #number of combinations tried\n",
    "        n_calls=50, #whats the best number for this?\n",
    "        #setting to 10 random guesses first, allowing exploration before making \"smarter decisions\"\n",
    "        n_random_starts=10,\n",
    "        #410 doesn't matter (just my bday). convention is 42.\n",
    "        #random state ensures that the random first combinations are the same for\n",
    "        #each run: pros\n",
    "            #Ensures repeatability (same experiment = same results).\n",
    "            # if results change, it's not due to randomness.\n",
    "            # Allows fair comparisons (changing only one parameter won't affect randomness).\n",
    "        #random_state=410\n",
    "    )\n",
    "    optimized_lighter_ingredients = result.x\n",
    "    print(\"Debug: Optimized lighter ingredients from Bayesian Optimization:\")\n",
    "    print(result.x)\n",
    "    \n",
    "    # Debug: Print the minimum error achieved\n",
    "    print(\"Debug: Minimum error achieved:\")\n",
    "    print(result.fun)\n",
    "    return optimized_lighter_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "396b4415-3e90-49a7-b9db-d3ce534ed46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eh = [7.441051483154297,\n",
    " 6.959020614624023,\n",
    " 5.859490871429443,\n",
    " 5.395391941070557,\n",
    " 3.0771231651306152,\n",
    " 0.9919356107711792]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0cf20bfb-2a9d-4f5d-98df-b689ddc5d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized heavy ingredients passed to bayesiann:\n",
      "[7.441051483154297, 6.959020614624023, 5.859490871429443, 5.395391941070557, 3.0771231651306152, 0.9919356107711792]\n",
      "Debug: Optimized lighter ingredients from Bayesian Optimization:\n",
      "[0.6363032108069293, 1.0, 0.705995480456912]\n",
      "Debug: Minimum error achieved:\n",
      "0.00011706091778351801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6363032108069293, 1.0, 0.705995480456912]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesiann(eh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "735b38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized heavy ingredients passed to bayesiann:\n",
      "[7.731051483154296, 7.049020614624023, 5.7694908714294435, 5.4853919410705565, 3.187123165130615, 0.8819356107711792]\n",
      "Debug: Optimized lighter ingredients from Bayesian Optimization:\n",
      "[1e-05, 1e-05, 1e-05]\n",
      "Debug: Minimum error achieved:\n",
      "0.7043763552547555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1e-05, 1e-05, 1e-05]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesiann(eh_w_noise)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79c88a42-9c49-4939-80c1-1e2903c88bf2",
   "metadata": {},
   "source": [
    "^^ With adding noise, there are a few issues:\n",
    "- if I add to an amount, then that could fill up a nutrient amount --> leaving no room for stuff like vanilla extract. However, if I change the noise to only remove values, this isn't really adding noise anymore and isn't random? Do I just subtract a certain amount from each value? So leaving room for everything. But then, do I subtract the same amount from each ingredient or subtract based on its position in the list (so lower index --> more subtracted as its assumed to have a larger amount)? Do we subtract a specific ratio of the ingredient? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a190437-1ea7-4b74-a7d2-1ef8535d9d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "edbe40f2-e6f9-4cd1-b4ab-cba14dc05129",
   "metadata": {},
   "source": [
    "Now, combining these methods into one function:\n",
    "- first get the heavy ingredients through gradient descent\n",
    "- then add noise to this optimized heavy list\n",
    "- then use Bayesian optimization to fine-tune the lighter ingredients\n",
    "- what's next? What if we put the final list through gradient descent again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "31103410-6ae3-4efc-be76-382af90b1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reversy(dictionary_list_for_heavy,target_dict,key_list):\n",
    "\n",
    "    optimized_heavy = running_tf(dictionary_list_for_heavy,target_dict,key_list)\n",
    "    noisy_optimized = add_noise(optimized_heavy)\n",
    "    lighter = bayesiann(noisy_optimized)\n",
    "\n",
    "    return optimized_heavy, lighter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9bbd76c6-de78-4a4b-b08b-12c9ad49dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6,) dtype=float32, numpy=\n",
      "array([7.640944 , 6.1839886, 5.6975045, 5.6498637, 2.257335 , 0.5703667],\n",
      "      dtype=float32)>\n",
      "28.000002\n",
      "Optimized heavy ingredients passed to bayesiann:\n",
      "[7.7850305938720705, 6.252763519287109, 5.657139053344727, 5.837734928131104, 2.373724689483643, 0.3693550431728363]\n",
      "Debug: Optimized lighter ingredients from Bayesian Optimization:\n",
      "[1e-05, 1.0, 1e-05]\n",
      "Debug: Minimum error achieved:\n",
      "0.007327536285742217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([7.7850305938720705,\n",
       "  6.252763519287109,\n",
       "  5.657139053344727,\n",
       "  5.837734928131104,\n",
       "  2.373724689483643,\n",
       "  0.3693550431728363],\n",
       " [1e-05, 1.0, 1e-05])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversy(heavy_dictionary_list,tates_dict,key_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a693c248-cc70-47d1-a79a-f4600ad45f33",
   "metadata": {},
   "source": [
    "trying something new: juice for life protein smoothie. this information is kind of old, as these were all ran before many changes were made.\n",
    "ALSO CARBOHYDRATES = NET CARBS + FIBER?\n",
    "SUGAR = ALL SUGAR????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dd0f02b1-df69-498e-995a-60a5c50a76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothie_dict = overall_dict.copy()\n",
    "smoothie_dict[\"Fat\"] = 10.1\n",
    "smoothie_dict[\"Saturated fatty acids\"] = 2.5\n",
    "smoothie_dict[\"Fatty acids, total trans\"] = 0\n",
    "smoothie_dict[\"Cholesterol\"] = 10/1000\n",
    "smoothie_dict[\"Sodium\"] = 428.7/1000\n",
    "smoothie_dict[\"Carbohydrate\"] = 92.2\n",
    "smoothie_dict[\"Fiber\"] = 10.2\n",
    "smoothie_dict[\"Sugars\"] = 66.9 + 37.5\n",
    "smoothie_dict[\"Protein\"] = 30.7\n",
    "smoothie_dict[\"Calcium\"] = 870/1000\n",
    "smoothie_dict[\"Iron\"] = 3/1000\n",
    "smoothie_dict[\"Potassium\"] = 1205.7/1000\n",
    "smoothie_dict[\"Vitamin D\"] = 0\n",
    "smoothie_dict[\"Weight\"] = 665.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "837ac46e-3e1c-45a6-8813-6609db02acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_almond_milk_file = \"vanilla_almond_milk_by_topco_associates_inc.csv\"\n",
    "vanilla_almond_milk_dict = overall_dict.copy()\n",
    "reader(vanilla_almond_milk_file,vanilla_almond_milk_dict,desired_serving)\n",
    "\n",
    "frozen_blueberries_file = \"blueberries_frozen.csv\"\n",
    "frozen_blueberries_dict = overall_dict.copy()\n",
    "reader(frozen_blueberries_file,frozen_blueberries_dict,desired_serving)\n",
    "\n",
    "frozen_strawberries_file = \"strawberries_frozen.csv\"\n",
    "frozen_strawberries_dict = overall_dict.copy()\n",
    "reader(frozen_strawberries_file,frozen_strawberries_dict,desired_serving)\n",
    "\n",
    "banana_file = \"banana_raw.csv\"\n",
    "banana_dict = overall_dict.copy()\n",
    "reader(banana_file,banana_dict,desired_serving)\n",
    "\n",
    "isopure_zero_carb_file = \"isopure_zero_carb_protein_creamy_vanilla.csv\"\n",
    "isopure_zero_carb_dict = overall_dict.copy()\n",
    "reader(isopure_zero_carb_file,isopure_zero_carb_dict,desired_serving)\n",
    "\n",
    "granola_homemade_file = \"granola_homemade.csv\"\n",
    "granola_homemade_dict = overall_dict.copy()\n",
    "reader(granola_homemade_file,granola_homemade_dict,desired_serving)\n",
    "\n",
    "honey_file = \"honey.csv\"\n",
    "honey_dict = overall_dict.copy()\n",
    "reader(honey_file,honey_dict,desired_serving)\n",
    "\n",
    "smoothie_ingredient_list = [vanilla_almond_milk_dict, frozen_blueberries_dict, frozen_strawberries_dict, banana_dict, isopure_zero_carb_dict, granola_homemade_dict, honey_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "99d5eb3b-1bdc-421e-9a3f-8923885c8d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(7,) dtype=float32, numpy=\n",
      "array([271.92798 , 196.73726 ,  69.600945,  65.63423 ,  35.12482 ,\n",
      "        19.13118 ,   6.943507], dtype=float32)>\n",
      "665.09985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[267.6542053222656,\n",
       " 193.63507080078125,\n",
       " 65.10347747802734,\n",
       " 64.10382843017578,\n",
       " 28.462602615356445,\n",
       " 21.039901733398438,\n",
       " 31.2152156829834]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_tf(smoothie_ingredient_list,smoothie_dict,key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4613104b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(7,) dtype=float32, numpy=\n",
      "array([151.26909 , 145.85268 , 141.86902 , 102.03573 ,  76.58154 ,\n",
      "        24.314064,  23.17786 ], dtype=float32)>\n",
      "665.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[152.32650756835938,\n",
       " 147.1532440185547,\n",
       " 142.36123657226562,\n",
       " 103.30818939208984,\n",
       " 57.0115966796875,\n",
       " 22.126388549804688,\n",
       " 39.401920318603516]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_tf(smoothie_ingredient_list,smoothie_dict,key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747ed94-aeb6-4a99-a247-3b7427320650",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [244,138,132,68,31,30,21]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
