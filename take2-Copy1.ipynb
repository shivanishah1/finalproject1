{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e2250611-102e-43d8-9848-71a66711385d",
   "metadata": {},
   "source": [
    "A bunch of libraries to import. The first four are standard regular ones that I've worked with before. The next few are ones I'm getting to know as the project continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec3ac28d-26f6-4ac2-9a8e-81667212816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb7920bd-cd06-436a-8ad3-7d358c02b9e7",
   "metadata": {},
   "source": [
    "creating the overall dictionary: the one representing the nutrition label facts. I will copy this dictionary for any ingredient I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6751668-92a8-4761-9c07-0cf9432fbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_dict = {}\n",
    "\n",
    "overall_dict[\"Fat\"] = 0\n",
    "overall_dict[\"Saturated fatty acids\"] = 0\n",
    "overall_dict[\"Fatty acids, total trans\"] = 0\n",
    "overall_dict[\"Cholesterol\"] = 0\n",
    "overall_dict[\"Sodium\"] = 0\n",
    "overall_dict[\"Carbohydrate\"] = 0\n",
    "overall_dict[\"Fiber\"] = 0\n",
    "overall_dict[\"Sugars\"] = 0\n",
    "overall_dict[\"Protein\"] = 0\n",
    "overall_dict[\"Calcium\"] = 0\n",
    "overall_dict[\"Iron\"] = 0\n",
    "overall_dict[\"Potassium\"] = 0\n",
    "overall_dict[\"Vitamin D\"] = 0\n",
    "overall_dict[\"Weight\"] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab94ec58-ef19-4883-98a6-c882f91c7095",
   "metadata": {},
   "source": [
    "nutritional information for tates cookies per serving (28 grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f647bf9-c62b-481b-a9f1-e694206ab906",
   "metadata": {},
   "outputs": [],
   "source": [
    "tates_dict = overall_dict.copy()\n",
    "tates_dict[\"Fat\"] = 7\n",
    "tates_dict[\"Saturated fatty acids\"] = 4.5\n",
    "tates_dict[\"Fatty acids, total trans\"] = 0\n",
    "tates_dict[\"Cholesterol\"] = .025\n",
    "tates_dict[\"Sodium\"] = .16\n",
    "tates_dict[\"Carbohydrate\"] = 18\n",
    "tates_dict[\"Fiber\"] = .8\n",
    "tates_dict[\"Sugars\"] = 12\n",
    "tates_dict[\"Protein\"] = 2\n",
    "tates_dict[\"Calcium\"] = .0000001\n",
    "tates_dict[\"Iron\"] = .0009\n",
    "tates_dict[\"Potassium\"] = .01\n",
    "tates_dict[\"Vitamin D\"] = .05\n",
    "tates_dict[\"Weight\"] = 28"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16d5f14f-c12e-4499-97d7-42520c4d28ff",
   "metadata": {},
   "source": [
    "reading in the information downloaded in csv format by nutritionvalue.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315b68bd-a761-4dea-a440-5830a52753b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_dict = overall_dict.copy()\n",
    "\n",
    "def reader(file,ingredient_dict,desired_serving):\n",
    "    with open(file, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for i,row in enumerate(csvreader):\n",
    "            if len(row) > 0 and row[0] in ingredient_dict:\n",
    "                if row[2] == \"mg\":\n",
    "                    ingredient_dict[row[0]] = (float(row[1]) / 1000)\n",
    "                elif row[2] == \"mcg\":\n",
    "                    ingredient_dict[row[0]] = (float(row[1]) / 1000000)\n",
    "                else: #grams\n",
    "                    ingredient_dict[row[0]] = (float(row[1]))\n",
    "            if i == 4:\n",
    "                index = row.index(\"g\")\n",
    "                ingredient_dict[\"Weight\"] = float(row[index-1])\n",
    "    return ingredient_dict"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12c2432f-e5bd-4b31-a1b0-c0f55e28ad09",
   "metadata": {},
   "source": [
    "the various ingredients in Tate's. Transforming the online nutrritional information of each ingredient into individual dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1058471-cc4e-4910-a56e-d9a8c3fd7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_sweet_chocolate_file = \"semisweet_chocolate_chips_by_raleys.csv\"\n",
    "semi_sweet_chocolate_dict = overall_dict.copy()\n",
    "reader(semi_sweet_chocolate_file,semi_sweet_chocolate_dict,desired_serving)\n",
    "\n",
    "unbleached_flour_file = \"flour_unbleached_enriched_allpurpose_wheat.csv\"\n",
    "unbleached_flour_dict = overall_dict.copy()\n",
    "reader(unbleached_flour_file,unbleached_flour_dict,desired_serving)\n",
    "\n",
    "salted_butter_file = \"butter_salted.csv\"\n",
    "salted_butter_dict = overall_dict.copy()\n",
    "reader(salted_butter_file,salted_butter_dict,desired_serving)\n",
    "\n",
    "cane_sugar_file = \"granulated_pure_cane_sugar.csv\"\n",
    "cane_sugar_dict = overall_dict.copy()\n",
    "reader(cane_sugar_file,cane_sugar_dict,desired_serving)\n",
    "\n",
    "brown_cane_sugar_file = \"brown_sugar_cane_by_frusecha.csv\"\n",
    "brown_cane_sugar_dict = overall_dict.copy()\n",
    "reader(brown_cane_sugar_file,brown_cane_sugar_dict,desired_serving)\n",
    "\n",
    "eggs_file = \"egg_fresh_raw_whole.csv\"\n",
    "eggs_dict = overall_dict.copy()\n",
    "reader(eggs_file,eggs_dict,desired_serving)\n",
    "\n",
    "baking_soda_file = \"leavening_agents_baking_soda.csv\"\n",
    "baking_soda_dict = overall_dict.copy()\n",
    "reader(baking_soda_file,baking_soda_dict,desired_serving)\n",
    "\n",
    "salt_file = \"salt_table.csv\"\n",
    "salt_dict = overall_dict.copy()\n",
    "reader(salt_file,salt_dict,desired_serving)\n",
    "\n",
    "natural_vanilla_flavor_file = \"vanilla_flavoring_syrup_by_r_torre__coinc.csv\"\n",
    "natural_vanilla_flavor_dict = overall_dict.copy()\n",
    "reader(natural_vanilla_flavor_file,natural_vanilla_flavor_dict,desired_serving)\n",
    "\n",
    "vanilla_extract_file = \"vanilla_extract.csv\"\n",
    "vanilla_extract_dict = overall_dict.copy()\n",
    "reader(vanilla_extract_file,vanilla_extract_dict,desired_serving)\n",
    "\n",
    "dictionary_list = [semi_sweet_chocolate_dict,unbleached_flour_dict,salted_butter_dict,cane_sugar_dict,brown_cane_sugar_dict,eggs_dict,baking_soda_dict,salt_dict,vanilla_extract_dict]\n",
    "heavy_dictionary_list = [semi_sweet_chocolate_dict,unbleached_flour_dict,salted_butter_dict,cane_sugar_dict,brown_cane_sugar_dict,eggs_dict]\n",
    "key_list = [\"Fat\",\"Saturated fatty acids\",\"Fatty acids, total trans\",\"Cholesterol\",\"Sodium\",\"Carbohydrate\",\"Fiber\",\"Sugars\",\"Protein\",\"Calcium\",\"Iron\",\"Potassium\",\"Vitamin D\",\"Weight\"]\n",
    "key_list_sodium = [\"Fat\",\"Saturated fatty acids\",\"Fatty acids, total trans\",\"Cholesterol\",\"Carbohydrate\",\"Fiber\",\"Sugars\",\"Protein\",\"Calcium\",\"Iron\",\"Potassium\",\"Vitamin D\",\"Weight\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c532bde4-9e1d-47a4-8af8-0f076106a939",
   "metadata": {},
   "source": [
    "just generates a list with length of inputted dictionary_list (so has a length of number of ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "635be9c7-5981-4067-a050-919369cfd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_x_list(number,dictionary_list):\n",
    "    x_list = []\n",
    "    for variable in range(0, len(dictionary_list)):\n",
    "        x_list.append(number)\n",
    "    return x_list\n",
    "\n",
    "x_list = building_x_list(1,dictionary_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e5ec123-1e93-4f6d-b2a0-318b24540711",
   "metadata": {},
   "source": [
    "function that evaluates the guesses and compares them with the goal values (nutritonal info for target product). Divided by 10 when evaluating because each dictionary is out of 10 grams. So if I had 20 grams of eggs, I'd multiply each nutritional fact by 2 (as each fact is per 10 grams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71dd96b6-b1f4-4153-aca4-22ec74dd5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equations(dictionary_list,x_list,desired_dict,key_list):\n",
    "    list_of_equations = []\n",
    "    for key in key_list:\n",
    "        equation = 0\n",
    "        for i,dictionary in enumerate(dictionary_list):\n",
    "            equation = equation + (dictionary[key] * (x_list[i] / 10))\n",
    "        list_of_equations.append(equation-(desired_dict[key]))\n",
    "    return list_of_equations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc5e9518-9e92-4271-8cd1-46702795ce2e",
   "metadata": {},
   "source": [
    "initializes variables, choosing random numbers between 0 and the largest weight possible. The variables all together should add up to the final weight. So first, I create these values. Then sort them in descending order. Then divide by a normalizing factor to ensure they add up to the final weight. This factor is just the weight / sum of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00a9a79a-8b3c-45bf-af72-8f3462bd4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_variables_tf(n,desired_dict):\n",
    "    largest_weight = desired_dict[\"Weight\"]\n",
    "\n",
    "    initial_values = tf.random.uniform(shape=(n,), minval=0.0, maxval=largest_weight)\n",
    "    initial_values = tf.sort(initial_values, direction='DESCENDING')\n",
    "\n",
    "    initial_values = (initial_values * largest_weight) / tf.reduce_sum(initial_values)\n",
    "    \n",
    "    return tf.Variable(initial_values, trainable=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87edc75d-c7a6-477c-8fc7-54782abd385f",
   "metadata": {},
   "source": [
    "this is the loss function. need to update this to provide more info.\n",
    "what can I add?\n",
    "- parameters: all variables are > 0, in descending order, equations --> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2c77b-f00b-4971-a70b-0eee2acc166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_tf(variables_tf,predicted):\n",
    "\n",
    "    error = tf.reduce_mean(tf.square(predicted))\n",
    "    error = error +  5*(tf.reduce_sum(tf.square(tf.minimum(variables_tf, 0))))\n",
    "    \n",
    "    variables_tf_array = variables_tf.numpy()\n",
    "    for i in range(0,len(variables_tf_array)-2):\n",
    "        if variables_tf_array[i] - variables_tf_array[i + 1] < 0:\n",
    "            error = error + ((variables_tf_array[i] - variables_tf_array[i + 1]) ** 2)\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2493d7d6-f1db-430c-91f9-12946c9bc1e8",
   "metadata": {},
   "source": [
    "now this is the training loop."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f27ebe8-c7b6-4a0c-b329-53704c901e6e",
   "metadata": {},
   "source": [
    "In this tates example, this issue comes up: chocolate chips overpowers the whole recipe.\n",
    "How do we change this:\n",
    "- restrict initial input of chocolate chips?\n",
    "- maintain a ratio?\n",
    "- standard deviation? \n",
    "\n",
    "\n",
    "could we change the learning rate as we go on? like the first few have a certain learning rate then we change it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "25e92179-5f5f-41c8-bb75-88c859639ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_tf(dictionary_list,target_dict,key_list):  \n",
    "    #number of ingredients\n",
    "    n = len(dictionary_list)\n",
    "    #first guess\n",
    "    variables_tf = initialize_variables_tf(n,target_dict)\n",
    "    print(variables_tf)\n",
    "    print(tf.reduce_sum(variables_tf).numpy())\n",
    "    #the optimizer chosen: stochastic gradient descent. WHAT IF WE TRIED batch gradient descent here?\n",
    "    #pros/cons: https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/\n",
    "    #whats the math behind this?\n",
    "        #updates each variable. the variable is updated to be its value - \n",
    "        #the learning rate * the gradient of how much that variable affects the loss.\n",
    "        #THIS PART IS DIFFERENT FROM MY PREVIOUS METHOD MATHEMATICALLY, but does the same thing\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=.01)\n",
    "\n",
    "# Training loop\n",
    "    #redefines the variable 400 times\n",
    "    for epoch in range(600):\n",
    "\n",
    "        #the \"with\" allows for automatic differentiation. this means the with part locks in the operations. but here no gradients are calculated.\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            predictions1 = equations(dictionary_list,variables_tf,target_dict,key_list)\n",
    "            predictions = tf.convert_to_tensor(predictions1)\n",
    "            loss = loss_tf(variables_tf,predictions)\n",
    "\n",
    "        #gradients are computed here.\n",
    "        gradients = tape.gradient(loss, [variables_tf])\n",
    "\n",
    "        #apply these gradients,perform sgd on our variables (which are being updated) \n",
    "        optimizer.apply_gradients(zip(gradients, [variables_tf]))\n",
    "\n",
    "        #if epoch % 20 == 0:\n",
    "         #   print((tf.reduce_sum(variables_tf).numpy()))\n",
    "        #    print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}, Variables: {variables_tf.numpy()}\")\n",
    "\n",
    "    return variables_tf.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2960c450-9a9c-4438-b0ba-8e4c197b6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6,) dtype=float32, numpy=\n",
      "array([6.8574424 , 6.3201056 , 5.168346  , 5.0650344 , 4.2372456 ,\n",
      "       0.35182598], dtype=float32)>\n",
      "28.0\n",
      "28.002226\n",
      "Epoch 0, Loss: 0.1289, Variables: [6.8579583  6.32034    5.1698413  5.064878   4.237122   0.35208455]\n",
      "28.042135\n",
      "Epoch 20, Loss: 0.1240, Variables: [6.8674717  6.3243556  5.1986675  5.0610747  4.233984   0.35658067]\n",
      "28.07469\n",
      "Epoch 40, Loss: 0.1197, Variables: [6.8756933  6.327295   5.2256327  5.0562763  4.2298436  0.35995087]\n",
      "28.101528\n",
      "Epoch 60, Loss: 0.1159, Variables: [6.8829193  6.3294086  5.250982   5.0508175  4.2250104  0.36239007]\n",
      "28.123894\n",
      "Epoch 80, Loss: 0.1124, Variables: [6.889376   6.330891   5.2749124  5.0449476  4.219717   0.36405084]\n",
      "28.142754\n",
      "Epoch 100, Loss: 0.1093, Variables: [6.895236  6.331889  5.2975826 5.0388517 4.2141395 0.3650527]\n",
      "28.158833\n",
      "Epoch 120, Loss: 0.1065, Variables: [6.900631   6.3325152  5.319119   5.0326695  4.2084074  0.36549032]\n",
      "28.172693\n",
      "Epoch 140, Loss: 0.1039, Variables: [6.9056616 6.332855  5.3396263 5.026498  4.202615  0.3654392]\n",
      "28.184776\n",
      "Epoch 160, Loss: 0.1015, Variables: [6.9104056  6.332975   5.359193   5.02041    4.1968327  0.36496013]\n",
      "28.195408\n",
      "Epoch 180, Loss: 0.0993, Variables: [6.914921   6.332925   5.377892   5.014457   4.191111   0.36410233]\n",
      "28.204851\n",
      "Epoch 200, Loss: 0.0973, Variables: [6.9192524  6.332744   5.395786   5.0086746  4.1854863  0.36290655]\n",
      "28.213305\n",
      "Epoch 220, Loss: 0.0954, Variables: [6.9234343  6.3324637  5.412929   5.0030866  4.1799846  0.36140633]\n",
      "28.220928\n",
      "Epoch 240, Loss: 0.0936, Variables: [6.927493   6.3321056  5.4293685  4.997709   4.1746206  0.35963008]\n",
      "28.22784\n",
      "Epoch 260, Loss: 0.0920, Variables: [6.931451   6.331688   5.4451466  4.9925485  4.169406   0.35760194]\n",
      "28.234144\n",
      "Epoch 280, Loss: 0.0905, Variables: [6.9353213  6.331226   5.4602995  4.987609   4.164346   0.35534266]\n",
      "28.239914\n",
      "Epoch 300, Loss: 0.0891, Variables: [6.939118   6.3307295  5.4748597  4.9828906  4.1594443  0.35287055]\n",
      "28.245213\n",
      "Epoch 320, Loss: 0.0878, Variables: [6.9428496  6.3302073  5.4888616  4.9783907  4.1547003  0.35020155]\n",
      "28.25009\n",
      "Epoch 340, Loss: 0.0866, Variables: [6.9465256 6.329668  5.502331  4.9741044 4.150112  0.3473501]\n",
      "28.254593\n",
      "Epoch 360, Loss: 0.0855, Variables: [6.95015   6.329116  5.515294  4.9700255 4.1456766 0.3443291]\n",
      "28.258745\n",
      "Epoch 380, Loss: 0.0844, Variables: [6.953728   6.328554   5.5277753  4.966149   4.1413903  0.34115025]\n",
      "28.262589\n",
      "Epoch 400, Loss: 0.0834, Variables: [6.9572644  6.3279915  5.5397973  4.962463   4.137249   0.33782434]\n",
      "28.266148\n",
      "Epoch 420, Loss: 0.0825, Variables: [6.9607615  6.327429   5.5513816  4.9589667  4.1332464  0.33436128]\n",
      "28.269434\n",
      "Epoch 440, Loss: 0.0816, Variables: [6.9642215 6.326866  5.5625477 4.955649  4.129379  0.3307702]\n",
      "28.272467\n",
      "Epoch 460, Loss: 0.0808, Variables: [6.9676476  6.3263035  5.573315   4.9525023  4.1256404  0.32705954]\n",
      "28.275272\n",
      "Epoch 480, Loss: 0.0800, Variables: [6.9710407  6.325746   5.583701   4.9495196  4.1220255  0.32323754]\n",
      "28.277853\n",
      "Epoch 500, Loss: 0.0792, Variables: [6.974403   6.3251944  5.593722   4.946693   4.1185293  0.31931156]\n",
      "28.28023\n",
      "Epoch 520, Loss: 0.0785, Variables: [6.977735   6.324651   5.6033936  4.944015   4.115146   0.31528872]\n",
      "28.282413\n",
      "Epoch 540, Loss: 0.0779, Variables: [6.9810386 6.3241167 5.612732  4.9414787 4.1118717 0.3111757]\n",
      "28.284414\n",
      "Epoch 560, Loss: 0.0772, Variables: [6.9843144  6.3235927  5.6217513  4.939078   4.1087008  0.30697876]\n",
      "28.286247\n",
      "Epoch 580, Loss: 0.0766, Variables: [6.987564   6.3230796  5.630466   4.936805   4.10563    0.30270392]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.990626811981201,\n",
       " 6.32260274887085,\n",
       " 5.638472557067871,\n",
       " 4.934759140014648,\n",
       " 4.102799415588379,\n",
       " 0.2985755205154419]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_tf(heavy_dictionary_list,tates_dict,key_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "feb0d3d9-669c-4e06-b5ff-0ba239f61c94",
   "metadata": {},
   "source": [
    "add noise to optimized heavy list to make room for lighter ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ffb859c-6f46-47b1-8a9c-ee4a176d687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(optimized_heavy_ingredients):\n",
    "    for i in range(0,len(optimized_heavy_ingredients)):\n",
    "        number = random.randint(1,2)\n",
    "        if number == 1:\n",
    "            optimized_heavy_ingredients[i] -= .01\n",
    "        else:\n",
    "            optimized_heavy_ingredients[i] += .01\n",
    "    return optimized_heavy_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eae8a859-98ba-478d-baa9-bb18411a0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = [6.990626811981201,\n",
    " 6.32260274887085,\n",
    " 5.638472557067871,\n",
    " 4.934759140014648,\n",
    " 4.102799415588379,\n",
    " 0.2985755205154419]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b05c6534-ae69-4109-9939-edbb61b2f747",
   "metadata": {},
   "source": [
    "error function for combined heavier + lighter ingredients. This looks at how the \"equations\" function evaluates as well as the calories. WOULD IT BE SMARTER TO MOVE THE CALORIES PORTION?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "26f76581-2589-43ce-bfc2-f214c32f3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function_lighter(lighter_ingredients,optimized_heavy_ingredients):\n",
    "\n",
    "    full_ingredients = np.concatenate((optimized_heavy_ingredients, lighter_ingredients))\n",
    "    equations_list = equations(dictionary_list,full_ingredients,tates_dict,key_list)\n",
    "    error = 0\n",
    "    for i, equation in enumerate(equations_list):\n",
    "        #if i == 4 or i == 5:\n",
    "        #    error += equation ** 2\n",
    "        #else:\n",
    "        error += equation\n",
    "\n",
    "    #gives the total carbs/fats/proteins as looks at the difference + targeted amount --> actual amount\n",
    "    carbs = equations_list[5] + tates_dict[\"Carbohydrate\"]\n",
    "    fats = equations_list[0] + tates_dict[\"Fat\"]\n",
    "    proteins = equations_list[8] + tates_dict[\"Protein\"]\n",
    "    calorie_count = (carbs * 4) + (fats * 9) + (proteins * 4)\n",
    "    #the 140 is tates specific\n",
    "    difference_in_calories = ((calorie_count) - 140) ** 2\n",
    "    error += (difference_in_calories / 100)   \n",
    "    #error /= len(equations_list)\n",
    "    return abs(error)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0a5ee62-fb09-4543-b67c-74c139c8ee61",
   "metadata": {},
   "source": [
    "Bayesian occurance. Samples points + finds the points that contribute to the least error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b617dee6-966d-48db-a23b-b424a921f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function_bayesian(lighter_ingredients, optimized_heavy_ingredients):\n",
    "    return error_function_lighter(lighter_ingredients, optimized_heavy_ingredients)\n",
    "\n",
    "# Map lighter ingredients to their indices\n",
    "lighter_indices = [6, 7, 8]  # vanilla, baking soda, xanthan gum\n",
    "\n",
    "# Define the search space for Bayesian Optimization using the indices\n",
    "search_space = [\n",
    "    Real(0.00001, 1, name=f\"ingredient_{lighter_indices[0]}\"),  # baking_soda\n",
    "    Real(0.00001, 1, name=f\"ingredient_{lighter_indices[1]}\"),  # salt\n",
    "    Real(0.00001, 1, name=f\"ingredient_{lighter_indices[2]}\")   # vanilla\n",
    "]\n",
    "\n",
    "def bayesiann(optimized_heavy_ingredients):\n",
    "    print(\"Optimized heavy ingredients passed to bayesiann:\")\n",
    "    print(optimized_heavy_ingredients)\n",
    "\n",
    "    error_function_fixed = partial(error_function_lighter, optimized_heavy_ingredients=optimized_heavy_ingredients)\n",
    "    result = gp_minimize(\n",
    "        func=error_function_fixed,\n",
    "        dimensions=search_space,\n",
    "        #number of combinations tried\n",
    "        n_calls=50,\n",
    "        #setting to 10 random guesses first, allowing exploration before making \"smarter decisions\"\n",
    "        n_random_starts=10,\n",
    "        #410 doesn't matter (just my bday). convention is 42.\n",
    "        #random state ensures that the random first combinations are the same for\n",
    "        #each run: pros\n",
    "            #Ensures repeatability (same experiment = same results).\n",
    "            # if results change, it's not due to randomness.\n",
    "            # Allows fair comparisons (changing only one parameter won't affect randomness).\n",
    "        #random_state=410\n",
    "    )\n",
    "    optimized_lighter_ingredients = result.x\n",
    "    print(\"Debug: Optimized lighter ingredients from Bayesian Optimization:\")\n",
    "    print(result.x)\n",
    "    \n",
    "    # Debug: Print the minimum error achieved\n",
    "    print(\"Debug: Minimum error achieved:\")\n",
    "    print(result.fun)\n",
    "    return optimized_lighter_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "396b4415-3e90-49a7-b9db-d3ce534ed46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eh = [7.7850305938720705, 6.252763519287109, 5.657139053344727, 5.837734928131104, 2.373724689483643, 0.3693550431728363]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0cf20bfb-2a9d-4f5d-98df-b689ddc5d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized heavy ingredients passed to bayesiann:\n",
      "[7.7850305938720705, 6.252763519287109, 5.657139053344727, 5.837734928131104, 2.373724689483643, 0.3693550431728363]\n",
      "Debug: Optimized lighter ingredients from Bayesian Optimization:\n",
      "[0.5357411429470648, 1e-05, 0.579714860265312]\n",
      "Debug: Minimum error achieved:\n",
      "0.0007517155807461179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5357411429470648, 1e-05, 0.579714860265312]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesiann(eh)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edbe40f2-e6f9-4cd1-b4ab-cba14dc05129",
   "metadata": {},
   "source": [
    "Now, combining these methods into one function:\n",
    "- first get the heavy ingredients through gradient descent\n",
    "- then add noise to this optimized heavy list\n",
    "- then use Bayesian optimization to fine-tune the lighter ingredients\n",
    "- what's next? What if we put the final list through gradient descent again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "31103410-6ae3-4efc-be76-382af90b1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reversy(dictionary_list_for_heavy,target_dict,key_list):\n",
    "\n",
    "    optimized_heavy = running_tf(dictionary_list_for_heavy,target_dict,key_list)\n",
    "    noisy_optimized = add_noise(optimized_heavy)\n",
    "    lighter = bayesiann(noisy_optimized)\n",
    "\n",
    "    return optimized_heavy, lighter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9bbd76c6-de78-4a4b-b08b-12c9ad49dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6,) dtype=float32, numpy=\n",
      "array([7.640944 , 6.1839886, 5.6975045, 5.6498637, 2.257335 , 0.5703667],\n",
      "      dtype=float32)>\n",
      "28.000002\n",
      "Optimized heavy ingredients passed to bayesiann:\n",
      "[7.7850305938720705, 6.252763519287109, 5.657139053344727, 5.837734928131104, 2.373724689483643, 0.3693550431728363]\n",
      "Debug: Optimized lighter ingredients from Bayesian Optimization:\n",
      "[1e-05, 1.0, 1e-05]\n",
      "Debug: Minimum error achieved:\n",
      "0.007327536285742217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([7.7850305938720705,\n",
       "  6.252763519287109,\n",
       "  5.657139053344727,\n",
       "  5.837734928131104,\n",
       "  2.373724689483643,\n",
       "  0.3693550431728363],\n",
       " [1e-05, 1.0, 1e-05])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversy(heavy_dictionary_list,tates_dict,key_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a693c248-cc70-47d1-a79a-f4600ad45f33",
   "metadata": {},
   "source": [
    "trying something new: juice for life protein smoothie (also for next steps want to try chocolate)\n",
    "ALSO CARBOHYDRATES = NET CARBS + FIBER?\n",
    "SUGAR = ALL SUGAR????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dd0f02b1-df69-498e-995a-60a5c50a76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothie_dict = overall_dict.copy()\n",
    "smoothie_dict[\"Fat\"] = 10.1\n",
    "smoothie_dict[\"Saturated fatty acids\"] = 2.5\n",
    "smoothie_dict[\"Fatty acids, total trans\"] = 0\n",
    "smoothie_dict[\"Cholesterol\"] = 10/1000\n",
    "smoothie_dict[\"Sodium\"] = 428.7/1000\n",
    "smoothie_dict[\"Carbohydrate\"] = 92.2\n",
    "smoothie_dict[\"Fiber\"] = 10.2\n",
    "smoothie_dict[\"Sugars\"] = 66.9 + 37.5\n",
    "smoothie_dict[\"Protein\"] = 30.7\n",
    "smoothie_dict[\"Calcium\"] = 870/1000\n",
    "smoothie_dict[\"Iron\"] = 3/1000\n",
    "smoothie_dict[\"Potassium\"] = 1205.7/1000\n",
    "smoothie_dict[\"Vitamin D\"] = 0\n",
    "smoothie_dict[\"Weight\"] = 665.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "837ac46e-3e1c-45a6-8813-6609db02acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_almond_milk_file = \"vanilla_almond_milk_by_topco_associates_inc.csv\"\n",
    "vanilla_almond_milk_dict = overall_dict.copy()\n",
    "reader(vanilla_almond_milk_file,vanilla_almond_milk_dict,desired_serving)\n",
    "\n",
    "frozen_blueberries_file = \"blueberries_frozen.csv\"\n",
    "frozen_blueberries_dict = overall_dict.copy()\n",
    "reader(frozen_blueberries_file,frozen_blueberries_dict,desired_serving)\n",
    "\n",
    "frozen_strawberries_file = \"strawberries_frozen.csv\"\n",
    "frozen_strawberries_dict = overall_dict.copy()\n",
    "reader(frozen_strawberries_file,frozen_strawberries_dict,desired_serving)\n",
    "\n",
    "banana_file = \"banana_raw.csv\"\n",
    "banana_dict = overall_dict.copy()\n",
    "reader(banana_file,banana_dict,desired_serving)\n",
    "\n",
    "isopure_zero_carb_file = \"isopure_zero_carb_protein_creamy_vanilla.csv\"\n",
    "isopure_zero_carb_dict = overall_dict.copy()\n",
    "reader(isopure_zero_carb_file,isopure_zero_carb_dict,desired_serving)\n",
    "\n",
    "granola_homemade_file = \"granola_homemade.csv\"\n",
    "granola_homemade_dict = overall_dict.copy()\n",
    "reader(granola_homemade_file,granola_homemade_dict,desired_serving)\n",
    "\n",
    "honey_file = \"honey.csv\"\n",
    "honey_dict = overall_dict.copy()\n",
    "reader(honey_file,honey_dict,desired_serving)\n",
    "\n",
    "smoothie_ingredient_list = [vanilla_almond_milk_dict, frozen_blueberries_dict, frozen_strawberries_dict, banana_dict, isopure_zero_carb_dict, granola_homemade_dict, honey_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "99d5eb3b-1bdc-421e-9a3f-8923885c8d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(7,) dtype=float32, numpy=\n",
      "array([271.92798 , 196.73726 ,  69.600945,  65.63423 ,  35.12482 ,\n",
      "        19.13118 ,   6.943507], dtype=float32)>\n",
      "665.09985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[267.6542053222656,\n",
       " 193.63507080078125,\n",
       " 65.10347747802734,\n",
       " 64.10382843017578,\n",
       " 28.462602615356445,\n",
       " 21.039901733398438,\n",
       " 31.2152156829834]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_tf(smoothie_ingredient_list,smoothie_dict,key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6b222ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(7,) dtype=float32, numpy=\n",
      "array([151.26909 , 145.85268 , 141.86902 , 102.03573 ,  76.58154 ,\n",
      "        24.314064,  23.17786 ], dtype=float32)>\n",
      "665.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[152.32650756835938,\n",
       " 147.1532440185547,\n",
       " 142.36123657226562,\n",
       " 103.30818939208984,\n",
       " 57.0115966796875,\n",
       " 22.126388549804688,\n",
       " 39.401920318603516]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_tf(smoothie_ingredient_list,smoothie_dict,key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747ed94-aeb6-4a99-a247-3b7427320650",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [244,138,132,68,31,30,21]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba953109-86c8-486a-b9d8-8518570dde7e",
   "metadata": {},
   "source": [
    "current issue: there are way too many local minimum. How do I find the one I want? \n",
    "- I could present all my local minimum possibilities... how to pick the certain one?\n",
    "- do I calculate a bunch and choose the one with lowest loss? Do I calculate and then find the most reoccuring one? This one appears the most naturally?\n",
    "- how can I incourporate food science ideas? Maybe I can add relationships between the ingredients. so if a food has more than a few sugar contributing ingredients, etc... what to do with that?\n",
    "- could I pass each guess through a \"viable food product\" type of function?\n",
    "    - processing-aware constraints, so there must be a specific flour to fat ratio for proper dough structure. or high sugar/fat ratio could cause spreading in cookies, etc. if something is very off the ratio?\n",
    "    - is there a way we could train the model with existing cookie recipes so that it can understand ratios? How do you even do this?\n",
    "    - put the guess through AI and have it say if its viable?\n",
    "    - maybe after each iteration, ask questions like\n",
    "        - does it have enough liquid? (ex. w the tates cookies, if it doesn't, then increase the eggs --> decrease the chocolate chips --> increase white/brown sugar)\n",
    "        - if the product has multiple ingredients that contribute to the same nutrient, ensuring that one doesn't hog all of it (assuming these are all heavier ingredients)\n",
    "        - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
