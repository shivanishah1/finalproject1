{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3ac28d-26f6-4ac2-9a8e-81667212816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 11:43:02.025896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6751668-92a8-4761-9c07-0cf9432fbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the overall dictionary: the one representing the nutrition label facts. I will copy this dictionary for any \n",
    "#ingredient I use.\n",
    "\n",
    "overall_dict = {}\n",
    "\n",
    "overall_dict[\"Fat\"] = 0\n",
    "overall_dict[\"Saturated fatty acids\"] = 0\n",
    "overall_dict[\"Fatty acids, total trans\"] = 0\n",
    "overall_dict[\"Cholesterol\"] = 0\n",
    "overall_dict[\"Sodium\"] = 0\n",
    "overall_dict[\"Carbohydrate\"] = 0\n",
    "overall_dict[\"Fiber\"] = 0\n",
    "overall_dict[\"Sugars\"] = 0\n",
    "overall_dict[\"Protein\"] = 0\n",
    "overall_dict[\"Calcium\"] = 0\n",
    "overall_dict[\"Iron\"] = 0\n",
    "overall_dict[\"Potassium\"] = 0\n",
    "overall_dict[\"Vitamin D\"] = 0\n",
    "overall_dict[\"Weight\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f647bf9-c62b-481b-a9f1-e694206ab906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nutritional information for tates cookies per 10 grams (each number was initially for  28 grams. So I had to multiply \n",
    "#each number by (10 * (1 / 28)) to get the information for 10 grams)\n",
    "\n",
    "tates_dict = overall_dict.copy()\n",
    "tates_dict[\"Fat\"] = 7\n",
    "tates_dict[\"Saturated fatty acids\"] = 4.5\n",
    "tates_dict[\"Fatty acids, total trans\"] = 0\n",
    "tates_dict[\"Cholesterol\"] = .025\n",
    "tates_dict[\"Sodium\"] = .16\n",
    "tates_dict[\"Carbohydrate\"] = 18\n",
    "tates_dict[\"Fiber\"] = .8\n",
    "tates_dict[\"Sugars\"] = 12\n",
    "tates_dict[\"Protein\"] = 2\n",
    "tates_dict[\"Calcium\"] = .0000001\n",
    "tates_dict[\"Iron\"] = .0009\n",
    "tates_dict[\"Potassium\"] = .01\n",
    "tates_dict[\"Vitamin D\"] = .05\n",
    "tates_dict[\"Weight\"] = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315b68bd-a761-4dea-a440-5830a52753b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_dict = overall_dict.copy()\n",
    "\n",
    "def reader(file,ingredient_dict,desired_serving):\n",
    "    with open(file, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for i,row in enumerate(csvreader):\n",
    "            if len(row) > 0 and row[0] in ingredient_dict:\n",
    "                if row[2] == \"mg\":\n",
    "                    ingredient_dict[row[0]] = (float(row[1]) / 1000)\n",
    "                elif row[2] == \"mcg\":\n",
    "                    ingredient_dict[row[0]] = (float(row[1]) / 1000000)\n",
    "                else: #grams\n",
    "                    ingredient_dict[row[0]] = (float(row[1]))\n",
    "            if i == 4:\n",
    "                index = row.index(\"g\")\n",
    "                ingredient_dict[\"Weight\"] = float(row[index-1])\n",
    "    return ingredient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1058471-cc4e-4910-a56e-d9a8c3fd7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the various ingredients in Tate's. Transforming the online nutrritional information of each ingredient into individual dictionaries.\n",
    "desired_serving = 1\n",
    "semi_sweet_chocolate_file = \"semisweet_chocolate_chips_by_raleys.csv\"\n",
    "semi_sweet_chocolate_dict = overall_dict.copy()\n",
    "reader(semi_sweet_chocolate_file,semi_sweet_chocolate_dict,desired_serving)\n",
    "\n",
    "unbleached_flour_file = \"flour_unbleached_enriched_allpurpose_wheat.csv\"\n",
    "unbleached_flour_dict = overall_dict.copy()\n",
    "reader(unbleached_flour_file,unbleached_flour_dict,desired_serving)\n",
    "\n",
    "salted_butter_file = \"butter_salted.csv\"\n",
    "salted_butter_dict = overall_dict.copy()\n",
    "reader(salted_butter_file,salted_butter_dict,desired_serving)\n",
    "\n",
    "cane_sugar_file = \"granulated_pure_cane_sugar.csv\"\n",
    "cane_sugar_dict = overall_dict.copy()\n",
    "reader(cane_sugar_file,cane_sugar_dict,desired_serving)\n",
    "\n",
    "brown_cane_sugar_file = \"brown_sugar_cane_by_frusecha.csv\"\n",
    "brown_cane_sugar_dict = overall_dict.copy()\n",
    "reader(brown_cane_sugar_file,brown_cane_sugar_dict,desired_serving)\n",
    "\n",
    "eggs_file = \"egg_fresh_raw_whole.csv\"\n",
    "eggs_dict = overall_dict.copy()\n",
    "reader(eggs_file,eggs_dict,desired_serving)\n",
    "\n",
    "baking_soda_file = \"leavening_agents_baking_soda.csv\"\n",
    "baking_soda_dict = overall_dict.copy()\n",
    "reader(baking_soda_file,baking_soda_dict,desired_serving)\n",
    "\n",
    "salt_file = \"salt_table.csv\"\n",
    "salt_dict = overall_dict.copy()\n",
    "reader(salt_file,salt_dict,desired_serving)\n",
    "\n",
    "natural_vanilla_flavor_file = \"vanilla_flavoring_syrup_by_r_torre__coinc.csv\"\n",
    "natural_vanilla_flavor_dict = overall_dict.copy()\n",
    "reader(natural_vanilla_flavor_file,natural_vanilla_flavor_dict,desired_serving)\n",
    "\n",
    "vanilla_extract_file = \"vanilla_extract.csv\"\n",
    "vanilla_extract_dict = overall_dict.copy()\n",
    "reader(vanilla_extract_file,vanilla_extract_dict,desired_serving)\n",
    "\n",
    "dictionary_list = [semi_sweet_chocolate_dict,unbleached_flour_dict,salted_butter_dict,cane_sugar_dict,brown_cane_sugar_dict,eggs_dict,baking_soda_dict,salt_dict,vanilla_extract_dict]\n",
    "heavy_dictionary_list = [semi_sweet_chocolate_dict,unbleached_flour_dict,salted_butter_dict,cane_sugar_dict,brown_cane_sugar_dict,eggs_dict]\n",
    "key_list = [\"Fat\",\"Saturated fatty acids\",\"Fatty acids, total trans\",\"Cholesterol\",\"Sodium\",\"Carbohydrate\",\"Fiber\",\"Sugars\",\"Protein\",\"Calcium\",\"Iron\",\"Potassium\",\"Vitamin D\",\"Weight\"]\n",
    "key_list_sodium = [\"Fat\",\"Saturated fatty acids\",\"Fatty acids, total trans\",\"Cholesterol\",\"Carbohydrate\",\"Fiber\",\"Sugars\",\"Protein\",\"Calcium\",\"Iron\",\"Potassium\",\"Vitamin D\",\"Weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635be9c7-5981-4067-a050-919369cfd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_x_list(number,dictionary_list):\n",
    "    x_list = []\n",
    "    for variable in range(0, len(dictionary_list)):\n",
    "        x_list.append(number)\n",
    "    return x_list\n",
    "\n",
    "x_list = building_x_list(1,dictionary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71dd96b6-b1f4-4153-aca4-22ec74dd5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equations(dictionary_list,x_list,desired_dict,key_list):\n",
    "    list_of_equations = []\n",
    "    for key in key_list:\n",
    "        equation = 0\n",
    "        for i,dictionary in enumerate(dictionary_list):\n",
    "            equation = equation + (dictionary[key] * (x_list[i]))\n",
    "        #subtracting the constant that the equation equals (a0x0 + a1x1 + ... + a13x13 = C1 -->  a0x0 + a1x1 + ... + a13x13 - C1)\n",
    "        list_of_equations.append(equation-(desired_dict[key]*desired_serving))\n",
    "    return list_of_equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ace2ec6-de2a-4f3c-a5a1-7fc706b9bf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fat': 2.667,\n",
       " 'Saturated fatty acids': 1.667,\n",
       " 'Fatty acids, total trans': 0.0,\n",
       " 'Cholesterol': 0.0,\n",
       " 'Sodium': 0.0,\n",
       " 'Carbohydrate': 6.67,\n",
       " 'Fiber': 0.67,\n",
       " 'Sugars': 5.33,\n",
       " 'Protein': 0.67,\n",
       " 'Calcium': 0.0,\n",
       " 'Iron': 0.00027,\n",
       " 'Potassium': 0.0,\n",
       " 'Vitamin D': 0,\n",
       " 'Weight': 10.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_sweet_chocolate_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "013b4c45-e4ce-426b-b20c-4e11e0a73aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0833070206299995,\n",
       " -0.10939591990999986,\n",
       " 0.028041673120000003,\n",
       " -0.021111773829,\n",
       " -0.153641972929,\n",
       " -0.11351416790000357,\n",
       " 0.7825712890000001,\n",
       " 0.5927771587000006,\n",
       " 0.04077140779999988,\n",
       " 0.001124447253,\n",
       " -9.590948539999985e-05,\n",
       " -0.005107948021,\n",
       " -0.049999988899564,\n",
       " -0.0707482000000006]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equations(heavy_dictionary_list,[2.3620467, 0.29056025, 0.08481605, 0, 0, 0.05550218],tates_dict,key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00a9a79a-8b3c-45bf-af72-8f3462bd4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_variables_tf(n):\n",
    "    # Random positive values \n",
    "    initial_values = tf.random.uniform(shape=(n,), minval=0.0, maxval=(1.8))\n",
    "    \n",
    "    # Sort in descending order\n",
    "    initial_values = tf.sort(initial_values, direction='DESCENDING')\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    #initial_values = initial_values / tf.reduce_sum(initial_values)\n",
    "    \n",
    "    return tf.Variable(initial_values, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59c2c77b-f00b-4971-a70b-0eee2acc166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_tf(predicted):\n",
    "    \n",
    "    return tf.reduce_mean(tf.square(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25e92179-5f5f-41c8-bb75-88c859639ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_tf(dictionary_list,target_dict):  \n",
    "    n = len(dictionary_list)\n",
    "    variables_tf = initialize_variables_tf(n)\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "    for epoch in range(400):  # Number of epochs\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "        # Step 1: Evaluate the equations\n",
    "            predictions1 = equations(dictionary_list,variables_tf,target_dict,key_list)\n",
    "\n",
    "            predictions = tf.convert_to_tensor(predictions1)\n",
    "\n",
    "        # Step 2: Compute the loss\n",
    "            loss = loss_tf(predictions)\n",
    "\n",
    "    # Step 3: Compute gradients\n",
    "        gradients = tape.gradient(loss, [variables_tf])\n",
    "\n",
    "    # Step 4: Apply gradients to update variables\n",
    "        optimizer.apply_gradients(zip(gradients, [variables_tf]))\n",
    "\n",
    "    # Apply constraints\n",
    "    # 1. Ensure all variables are positive\n",
    "        variables_tf.assign(tf.maximum(variables_tf, 0.0))\n",
    "    \n",
    "    # 2. Ensure variables are in descending order\n",
    "        #variables_tf.assign(tf.sort(variables_tf, direction='DESCENDING'))\n",
    "\n",
    "        if epoch % 20 == 0:  # Print every 20 epochs\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}, Variables: {variables_tf.numpy()}\")\n",
    "\n",
    "    x_for_one_serving = []\n",
    "    for x in variables_tf.numpy():\n",
    "        x_for_one_serving.append(x * 270)\n",
    "    print(x_for_one_serving)\n",
    "\n",
    "    return (variables_tf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2960c450-9a9c-4438-b0ba-8e4c197b6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 53.2514, Variables: [0.86856186 0.7961991  0.37248674 0.09400445 0.         0.1317291 ]\n",
      "Epoch 20, Loss: 0.0708, Variables: [1.050708   0.6972396  0.45294484 0.40074867 0.22630641 0.00644752]\n",
      "Epoch 40, Loss: 0.0326, Variables: [1.0676028  0.630691   0.47778893 0.42146996 0.22819506 0.        ]\n",
      "Epoch 60, Loss: 0.0276, Variables: [1.0786926  0.6051593  0.48196617 0.43051597 0.22490819 0.        ]\n",
      "Epoch 80, Loss: 0.0263, Variables: [1.0882632  0.5953169  0.48191208 0.43449643 0.21935199 0.        ]\n",
      "Epoch 100, Loss: 0.0256, Variables: [1.0971568  0.59127057 0.48036462 0.43659166 0.21303512 0.        ]\n",
      "Epoch 120, Loss: 0.0249, Variables: [1.1056828  0.58937085 0.47830105 0.43798694 0.20652089 0.        ]\n",
      "Epoch 140, Loss: 0.0244, Variables: [1.1139582  0.5882726  0.47608042 0.43911925 0.20001535 0.        ]\n",
      "Epoch 160, Loss: 0.0238, Variables: [1.1220281  0.58747977 0.47383448 0.44014966 0.19359271 0.        ]\n",
      "Epoch 180, Loss: 0.0232, Variables: [1.1299117  0.5868093  0.47161093 0.4411376  0.18727863 0.        ]\n",
      "Epoch 200, Loss: 0.0227, Variables: [1.1376184  0.5861935  0.46942684 0.44210526 0.18108112 0.        ]\n",
      "Epoch 220, Loss: 0.0222, Variables: [1.145154   0.5856073  0.46728772 0.44306046 0.1750012  0.        ]\n",
      "Epoch 240, Loss: 0.0217, Variables: [1.1525224  0.5850411  0.46519518 0.44400662 0.1690383  0.        ]\n",
      "Epoch 260, Loss: 0.0213, Variables: [1.1597282  0.5844912  0.46314874 0.44494462 0.16318999 0.        ]\n",
      "Epoch 280, Loss: 0.0208, Variables: [1.1667747  0.5839563  0.461148   0.44587526 0.15745445 0.        ]\n",
      "Epoch 300, Loss: 0.0204, Variables: [1.1736653  0.58343536 0.45919192 0.44679856 0.15182914 0.        ]\n",
      "Epoch 320, Loss: 0.0200, Variables: [1.1804034  0.58292794 0.45727974 0.44771478 0.14631195 0.        ]\n",
      "Epoch 340, Loss: 0.0196, Variables: [1.1869922  0.58243376 0.45541042 0.4486239  0.14090061 0.        ]\n",
      "Epoch 360, Loss: 0.0193, Variables: [1.1934352  0.58195245 0.45358315 0.4495262  0.135593   0.        ]\n",
      "Epoch 380, Loss: 0.0189, Variables: [1.1997349  0.58148366 0.4517969  0.4504216  0.1303868  0.        ]\n",
      "[325.50936698913574, 156.88344597816467, 121.53706759214401, 121.84181749820709, 33.89393404126167, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.2055902 , 0.5810498 , 0.4501373 , 0.451266  , 0.12553309,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_tf(heavy_dictionary_list,tates_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00860cfc-c7ea-4a45-a062-33cafd45fe69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
